{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c6a138-d6c1-4116-a195-7e92d77ce712",
   "metadata": {},
   "source": [
    "# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ë„¤ì´ë²„ ê²€ìƒ‰ì„ í†µí•´ ì¼ìë³„ë¡œ ìˆ˜ì§‘ëœ URL ëª©ë¡ì„ ì´ìš©í•´ ì‹¤ì œ ë¸”ë¡œê·¸ì˜ ì œëª©ê³¼ ë³¸ë¬¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì²´ê³„ì ì¸ ë‹¨ê³„ë¥¼ ê±°ì³ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71202870-1ca7-43ee-bf40-052ae10af2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427bff-6fc6-4a85-8a71-711f6fec3346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. ë‚ ì§œ í´ë”ì— í©ì–´ì ¸ ìˆëŠ” íŒŒì¼ ë³‘í•©(ì¤‘ë³µ ì œê±° - ë§í¬ ê¸°ì¤€)\n",
    "- ëª©í‘œ : ì¼ìë³„ í´ë”ì— ë“±ë¡ë˜ì–´ ìˆëŠ” '8ê°œ í‚¤ì›Œë“œ(ì¹´ì¹´ì˜¤í†¡ ì—…ë°ì´íŠ¸, ì¹´ì¹´ì˜¤í†¡ ê°œí¸, ì¹´í†¡ ì—…ë°ì´íŠ¸, ì¹´í†¡ ê°œí¸, ë„¤ì´íŠ¸ì˜¨, ë¼ì¸, ì¹´ì¹´ì˜¤í†¡, ì¹´í†¡)'ë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ íŒŒì¼ë“¤ì„ URL ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ì˜ ë°ì´í„° í”„ë ˆì„ì— í†µí•©í•©ë‹ˆë‹¤.\n",
    "- í•µì‹¬ : set() ìë£Œêµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬, ì—¬ëŸ¬ ë²ˆ ìˆ˜ì§‘ë˜ì—ˆì„ ìˆ˜ ìˆëŠ” ì¤‘ë³µ URLì„ ì´ ë‹¨ê³„ì—ì„œ ë¯¸ë¦¬ ì œê±°í•˜ì—¬ ë°ì´í„°ì˜ ì •í•©ì„±ì„ í™•ë³´í•˜ê³ , ì´í›„ì˜ í¬ë¡¤ë§ ì‘ì—… íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa88aa-3e23-4bbd-9c3f-346a3a37fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# ì¼ìë³„ë¡œ ìˆ˜ì§‘ëœ ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì§€ì •(load file)\n",
    "BASE_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/data'\n",
    "# ê²°ê³¼ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì§€ì •\n",
    "OUTPUT_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/merged_data_blog'\n",
    "\n",
    "# ìˆ˜ì§‘ëœ íŒŒì¼ëª…(ì˜ˆ: '251005_naver_blogs_links_final1')ì— í‚¤ì›Œë“œ ëŒ€ì‹  ìˆ«ìë¥¼ ì…ë ¥í–ˆì—ˆê¸° ë•Œë¬¸ì— ì‹¤ì œ ì–´ë–¤ í‚¤ì›Œë“œë¥¼ ì˜ë¯¸í•˜ëŠ”ì§€ë¥¼ ë§¤ì¹­ì‹œí‚¤ê¸° \n",
    "keyword_map = {\n",
    "    1: 'ì¹´ì¹´ì˜¤í†¡_ì—…ë°ì´íŠ¸',\n",
    "    2: 'ì¹´ì¹´ì˜¤í†¡_ê°œí¸',\n",
    "    3: 'ì¹´í†¡_ì—…ë°ì´íŠ¸',\n",
    "    4: 'ì¹´í†¡_ê°œí¸',\n",
    "    5: 'ë„¤ì´íŠ¸ì˜¨',\n",
    "    6: 'ë¼ì¸',\n",
    "    7: 'ì¹´ì¹´ì˜¤í†¡',\n",
    "    8: 'ì¹´í†¡'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘ ë° í‚¤ì›Œë“œë³„ ê·¸ë£¹í™”\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"íŒŒì¼ ê²€ìƒ‰ ì‹œì‘\")\n",
    "\n",
    "# í‚¤ì›Œë“œ ë²ˆí˜¸(1~8)ë³„ë¡œ íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "file_groups = {num: [] for num in range(1, 9)}\n",
    "\n",
    "# BASE_PATH ì•„ë˜ ë‚ ì§œ í´ë”ê°€ ìˆê¸° ë•Œë¬¸ì— ë‚´ë¶€ í´ë” ê²€ìƒ‰í•˜ê²Œ ë°˜ë³µë¬¸ ì ìš©\n",
    "for dir_name in os.listdir(BASE_PATH):\n",
    "    dir_path = os.path.join(BASE_PATH, dir_name)\n",
    "    \n",
    "    if os.path.isdir(dir_path):\n",
    "        # í´ë” ë‚´ íŒŒì¼ ì²´í¬\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            # 'final'ë¡œ ëë‚˜ê³  '.csv' í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ë§Œ ëŒ€ìƒ\n",
    "            if 'final' in file_name and file_name.endswith('.csv'):\n",
    "                try:\n",
    "                    # íŒŒì¼ëª…ì—ì„œ ìˆ«ì ë¶€ë¶„(e.g., 'final1' -> 1) ì¶”ì¶œ\n",
    "                    file_num = int(file_name.split('final')[1].split('.csv')[0])\n",
    "                    if file_num in file_groups:\n",
    "                        full_path = os.path.join(dir_path, file_name)\n",
    "                        file_groups[file_num].append(full_path)\n",
    "                except (ValueError, IndexError):\n",
    "                    # íŒŒì¼ëª… í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ í›„ íŒ¨ìŠ¤\n",
    "                    print(f\"íŒŒì¼ëª… í˜•ì‹ì´ ë‹¬ë¼ ê±´ë„ˆëœë‹ˆë‹¤: {file_name}\")\n",
    "\n",
    "print(\"íŒŒì¼ ê·¸ë£¹í™” ì™„ë£Œ\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. ê·¸ë£¹ë³„ íŒŒì¼ ë³‘í•©, ì¤‘ë³µ ì œê±° ë° ì €ì¥\n",
    "# --------------------------------------------------------------------------\n",
    "# ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(f\"'{OUTPUT_PATH}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê·¸ë£¹í™”ëœ íŒŒì¼ë“¤ì„ ë³‘í•© ì‘ì—…í•˜ê¸°\n",
    "for num, file_list in file_groups.items():\n",
    "    if not file_list:\n",
    "        print(f\"í‚¤ì›Œë“œ ê·¸ë£¹ {num} ({keyword_map[num]})ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "\n",
    "    # ê° ê·¸ë£¹ì˜ ëª¨ë“  CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ì–´ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê³  í•©ì¹˜ê¸°\n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # 'URL' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µëœ í–‰ ì œê±°(ì²«ë²ˆì§¸ íŒŒì¼ë§Œ ë‚¨ê¹€)\n",
    "    deduplicated_df = combined_df.drop_duplicates(subset=['URL'], keep='first')\n",
    "    \n",
    "    # ê²°ê³¼ íŒŒì¼ì„ CSVë¡œ ì €ì¥(ì¸ì½”ë”© ê¼­ ì ê¸°!!)\n",
    "    output_filename = f\"combined_{num}_{keyword_map[num]}.csv\"\n",
    "    output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
    "    deduplicated_df.to_csv(output_filepath, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # ì‘ì—… ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"'{output_filename}' íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\" - í†µí•© ì „ ì´ ë°ì´í„° ìˆ˜: {len(combined_df)}ê°œ\")\n",
    "    print(f\" - ì¤‘ë³µ ì œê±° í›„ ì´ ë°ì´í„° ìˆ˜: {len(deduplicated_df)}ê°œ\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee80ed0-530f-4779-9167-708a3fbebd29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. ë³‘í•©ëœ íŒŒì¼ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” ë¸”ë¡œê·¸ ì œì™¸í•˜ê¸°\n",
    "- ëª©í‘œ: ë¶„ì„ê³¼ ê´€ë ¨ ì—†ëŠ” ë°ì´í„°ë¥¼ ì œê±°í•˜ì—¬ ë¶„ì„ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
    "- í•µì‹¬: ì œëª©ì— 'ì„¤ì •í•œ í•µì‹¬ í‚¤ì›Œë“œ'ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” í¬ìŠ¤íŠ¸ë¥¼ ë³¸ë¬¸ ìˆ˜ì§‘ ì „ì— í•„í„°ë§í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c2cdd-b5de-4333-85c8-8e9435f493c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 1ë‹¨ê³„ ê²°ê³¼ í´ë” ì—°ê²°\n",
    "INPUT_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/merged_data_blog'\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì§€ì •\n",
    "FILTERED_OUTPUT_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/filtered_data_blog'\n",
    "\n",
    "# 'ì œëª©' ì»¬ëŸ¼ì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì œê±°\n",
    "KEYWORDS_TO_CHECK = [\"ì¹´ì¹´ì˜¤í†¡\", \"ì—…ë°ì´íŠ¸\", \"ê°œí¸\", \"ì¹´í†¡\", \"ë„¤ì´íŠ¸ì˜¨\", \"ë¼ì¸\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. í´ë” ìƒì„± ë° í•„í„°ë§ ì‘ì—… ì‹œì‘\n",
    "# --------------------------------------------------------------------------\n",
    "# ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "if not os.path.exists(FILTERED_OUTPUT_PATH):\n",
    "    os.makedirs(FILTERED_OUTPUT_PATH)\n",
    "    print(f\"'{FILTERED_OUTPUT_PATH}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. íŒŒì¼ ì²˜ë¦¬ ë° í•„í„°ë§\n",
    "# --------------------------------------------------------------------------\n",
    "# INPUT_PATH í´ë”ì— ìˆëŠ” ëª¨ë“  íŒŒì¼(csv íŒŒì¼ë§Œ ëŒ€ìƒìœ¼ë¡œ) ì²´í¬\n",
    "for filename in os.listdir(INPUT_PATH):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(INPUT_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            original_rows = len(df)\n",
    "            \n",
    "            # 'ì œëª©' ì»¬ëŸ¼ì´ ìˆëŠ” ì§€ ì²´í¬\n",
    "            if 'ì œëª©' not in df.columns:\n",
    "                print(f\"'{filename}' íŒŒì¼ì— 'ì œëª©' ì»¬ëŸ¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                continue\n",
    "                \n",
    "            # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ OR ì¡°ê±´ìœ¼ë¡œ ë¬¶ì–´ í•˜ë‚˜ì˜ ê²€ìƒ‰ íŒ¨í„´ìœ¼ë¡œ ìƒì„±\n",
    "            search_pattern = '|'.join(KEYWORDS_TO_CHECK)\n",
    "            \n",
    "            # 'ì œëª©' ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œ íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ëª¨ë“  í–‰ ì¶”ì¶œ\n",
    "            # na=False ì˜µì…˜ì€ 'ì œëª©'ì´ ë¹„ì–´ìˆëŠ”(NaN) ê²½ìš° Falseë¡œ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•¨\n",
    "            filtered_df = df[df['ì œëª©'].str.contains(search_pattern, na=False)].copy()\n",
    "            \n",
    "            # í•„í„°ë§ í›„ ë‚¨ì€ ë°ì´í„°ì˜ í–‰ ê°œìˆ˜ ì €ì¥\n",
    "            filtered_rows = len(filtered_df)\n",
    "            \n",
    "            # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n",
    "            output_filename = f\"filtered_{filename}\"\n",
    "            output_filepath = os.path.join(FILTERED_OUTPUT_PATH, output_filename)\n",
    "            filtered_df.to_csv(output_filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # ì‘ì—… ê²°ê³¼ ì¶œë ¥\n",
    "            print(f\"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {filename}\")\n",
    "            print(f\" - ì›ë³¸ ë°ì´í„° ìˆ˜: {original_rows}ê°œ\")\n",
    "            print(f\" - í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {filtered_rows}ê°œ\")\n",
    "            print(f\" - ê²°ê³¼ ì €ì¥ íŒŒì¼: {output_filename}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        except Exception as e:\n",
    "            # íŒŒì¼ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥\n",
    "            print(f\"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "print(\"ëª¨ë“  í•„í„°ë§ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db6c41-8b35-4f12-bb25-7dd12c06dc77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. íŒŒì¼ ë‚´ ë§í¬ë¡œ ì´ë™í•´ í•´ë‹¹ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ ë‚´ìš©ê³¼ ì‘ì„±ì¼ì„ í¬ë¡¤ë§í•˜ê¸°(iframe ì´ìŠˆ ì²˜ë¦¬)\n",
    "- ëª©í‘œ: ì •ì œëœ URL ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹¤ì œ ë¶„ì„ì˜ ëŒ€ìƒì´ ë  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "- í•µì‹¬: requestsì™€ BeautifulSoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë³¸ë¬¸ ì˜ì—­ì„ íƒ€ê²ŸíŒ…í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê³  IP ì°¨ë‹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê° ìš”ì²­ ì‚¬ì´ì— time.sleep(1)ì„ ì ìš©í•˜ëŠ” 'Polite Scraping' ì›ì¹™ì„ ì¤€ìˆ˜í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24663336-6ab9-48d3-835b-a3f147245863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 2ë‹¨ê³„ ê²°ê³¼ í´ë” ì—°ê²°\n",
    "INPUT_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/filtered_data_blog'\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì§€ì •\n",
    "CRAWLED_OUTPUT_PATH = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/collected_main_text_blog'\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥ ì£¼ê¸°\n",
    "SAVE_INTERVAL = 50\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. í¬ë¡¤ë§ í•¨ìˆ˜(ë‚ ì§œ, ë³¸ë¬¸ ë™ì‹œ ìˆ˜ì§‘)\n",
    "# --------------------------------------------------------------------------\n",
    "def scrape_blog_content(url):\n",
    "    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ë°›ì•„ (ì‘ì„±ì¼, ë³¸ë¬¸ë‚´ìš©) íŠœí”Œì„ ë°˜í™˜\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                          'Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # 1. iframe í™•ì¸\n",
    "        initial_response = requests.get(url, headers=headers, timeout=10)\n",
    "        if initial_response.status_code != 200:\n",
    "            return None, f\"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (1ë‹¨ê³„): {initial_response.status_code}\"\n",
    "            \n",
    "        initial_soup = BeautifulSoup(initial_response.text, 'html.parser')\n",
    "        main_frame = initial_soup.find('iframe', id='mainFrame')\n",
    "        \n",
    "        # iframeì´ ìˆëŠ” ê²½ìš° - ëŒ€ë¶€ë¶„ì˜ ë„¤ì´ë²„ ë¸”ë¡œê·¸\n",
    "        if main_frame:\n",
    "            real_blog_url = \"https://blog.naver.com\" + main_frame['src']\n",
    "            content_response = requests.get(real_blog_url, headers=headers, timeout=10)\n",
    "            if content_response.status_code != 200:\n",
    "                return None, f\"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (2ë‹¨ê³„): {content_response.status_code}\"\n",
    "            \n",
    "\n",
    "            soup = BeautifulSoup(content_response.text, 'html.parser')\n",
    "        else:\n",
    "            # iframeì´ ì—†ëŠ” íŠ¹ì´ ì¼€ì´ìŠ¤ - ëª¨ë°”ì¼ ë·° ë“±\n",
    "            soup = initial_soup\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (1) ë‚ ì§œ ì¶”ì¶œ ë¡œì§\n",
    "        # ---------------------------------------\n",
    "        date_text = None\n",
    "        # ìµœì‹  ì—ë””í„°\n",
    "        date_elem = soup.find('span', class_='se_publishDate')\n",
    "        # êµ¬ë²„ì „ ì—ë””í„°\n",
    "        if not date_elem:\n",
    "            date_elem = soup.find('span', class_='date')\n",
    "        if not date_elem:\n",
    "            date_elem = soup.find('p', class_='date')\n",
    "            \n",
    "        if date_elem:\n",
    "            date_text = date_elem.get_text(strip=True)\n",
    "        else:\n",
    "            date_text = \"ë‚ ì§œ ë¯¸ìƒ\"\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (2) ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§\n",
    "        # ---------------------------------------\n",
    "        content_text = \"\"\n",
    "        main_container = soup.find('div', class_='se-main-container')\n",
    "        \n",
    "        if main_container:\n",
    "            content_text = main_container.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            post_view_area = soup.find('div', id='postViewArea')\n",
    "            if post_view_area:\n",
    "                content_text = post_view_area.get_text(separator='\\n', strip=True)\n",
    "            else:\n",
    "                content_text = \"\" # ë³¸ë¬¸ì„ ëª» ì°¾ì€ ê²½ìš° ë¹ˆ ë¬¸ìì—´\n",
    "\n",
    "        # ë‚ ì§œ, ë³¸ë¬¸ ë¹ˆí™˜\n",
    "        return date_text, content_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None, f\"í¬ë¡¤ë§ ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
    "        \n",
    "# --------------------------------------------------------------------------\n",
    "# 3. í´ë” ìƒì„± ë° í¬ë¡¤ë§ ì‘ì—… ì‹œì‘\n",
    "# --------------------------------------------------------------------------\n",
    "if not os.path.exists(CRAWLED_OUTPUT_PATH):\n",
    "    os.makedirs(CRAWLED_OUTPUT_PATH)\n",
    "    print(f\"'{CRAWLED_OUTPUT_PATH}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ë¸”ë¡œê·¸ ì‘ì„±ì¼ ë° ë³¸ë¬¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í™œì„±í™”)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. íŒŒì¼ ìˆœíšŒ ë° í¬ë¡¤ë§ ì‹¤í–‰ (ì‘ì„±ì¼/ë³¸ë¬¸ ë™ì‹œ ì €ì¥)\n",
    "# --------------------------------------------------------------------------\n",
    "all_files = [f for f in os.listdir(INPUT_PATH) if f.endswith('.csv')]\n",
    "\n",
    "for filename in all_files:\n",
    "    print(f\"[{filename}] íŒŒì¼ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # ì›ë³¸ íŒŒì¼ ê²½ë¡œì™€ ìµœì¢… ì €ì¥ë  íŒŒì¼(.pkl) ê²½ë¡œ\n",
    "    source_file_path = os.path.join(INPUT_PATH, filename)\n",
    "    output_filename = f\"crawled_{os.path.splitext(filename)[0]}.pkl\"\n",
    "    output_filepath = os.path.join(CRAWLED_OUTPUT_PATH, output_filename)\n",
    "\n",
    "    # --- ì´ì–´í•˜ê¸° ---\n",
    "    if os.path.exists(output_filepath):\n",
    "        print(f\" -> '{output_filename}' íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì‘ì—…ì„ ì´ì–´ì„œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        df = pd.read_pickle(output_filepath)\n",
    "    else:\n",
    "        print(f\" -> ìƒˆë¡œìš´ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        df = pd.read_csv(source_file_path)\n",
    "    \n",
    "    # 'ë³¸ë¬¸' ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "    if 'ë³¸ë¬¸' not in df.columns:\n",
    "        df['ë³¸ë¬¸'] = np.nan\n",
    "        \n",
    "    # 'ì‘ì„±ì¼' ì»¬ëŸ¼ë„ ì´ˆê¸°í™”\n",
    "    if 'ì‘ì„±ì¼' not in df.columns:\n",
    "        df['ì‘ì„±ì¼'] = np.nan\n",
    "\n",
    "    # í¬ë¡¤ë§ì´ ì•„ì§ ë˜ì§€ ì•Šì€ í–‰ë“¤ë§Œ ëŒ€ìƒ - ë³¸ë¬¸ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°\n",
    "    to_crawl = df[df['ë³¸ë¬¸'].isnull()]\n",
    "    \n",
    "    if to_crawl.empty:\n",
    "        print(\" -> ëª¨ë“  URLì˜ ìˆ˜ì§‘ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(\"-\" * 50)\n",
    "        continue\n",
    "\n",
    "    print(f\" -> ì´ {len(df)}ê°œ ì¤‘ {len(to_crawl)}ê°œì˜ URLì— ëŒ€í•œ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì§„í–‰ë¥  í‘œì‹œ\n",
    "    for index, row in tqdm(to_crawl.iterrows(), total=len(to_crawl)):\n",
    "        date_val, content_val = scrape_blog_content(row['URL'])\n",
    "        \n",
    "        df.loc[index, 'ì‘ì„±ì¼'] = date_val\n",
    "        df.loc[index, 'ë³¸ë¬¸'] = content_val\n",
    "        \n",
    "        # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # --- ì¤‘ê°„ ì €ì¥ ---\n",
    "        current_progress_index = to_crawl.index.get_loc(index)\n",
    "        if (current_progress_index + 1) % SAVE_INTERVAL == 0:\n",
    "            df.to_pickle(output_filepath)\n",
    "            tqdm.write(f\" -> ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({current_progress_index + 1}/{len(to_crawl)} ì§„í–‰)\")\n",
    "\n",
    "    # ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„ ìµœì¢… ì €ì¥\n",
    "    df.to_pickle(output_filepath)\n",
    "    print(f\" -> ì™„ë£Œ! ìµœì¢… ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494a989-f8e7-47d1-bb1a-08ddc1290a33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. ìˆ˜ì§‘ ì™„ë£Œ í›„ ê²°ê³¼ í™•ì¸\n",
    "- ëª©í‘œ : ìˆ˜ì§‘ëœ ê²°ê³¼ë¬¼ ì¤‘ ìˆ˜ì§‘ ì‹¤íŒ¨ ë° ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì§€ ì•Šì€ í•­ëª©ì„ í™•ì¸í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996b776-0c02-403b-b672-0e863e94b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ ì œëŒ€ë¡œ ì•ˆëœ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. ë¶„ì„í•  í´ë” ê²½ë¡œ - 3ë²ˆ ì‘ì—… ê²°ê³¼\n",
    "# --------------------------------------------------------------------------\n",
    "TARGET_PATH = r'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/collected_main_text_blog'\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "# --------------------------------------------------------------------------\n",
    "total_blog_count = 0      # 1. ì´ ìˆ˜ì§‘ ë¸”ë¡œê·¸ ê°œìˆ˜\n",
    "total_fail_count = 0      # 2. í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ ê°œìˆ˜ (1, 2ë‹¨ê³„ í¬í•¨)\n",
    "total_unknown_date = 0    # 3. ë‚ ì§œ ë¯¸ìƒ ê°œìˆ˜\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. íŒŒì¼ ìˆœíšŒ ë° ë¶„ì„\n",
    "# --------------------------------------------------------------------------\n",
    "if not os.path.exists(TARGET_PATH):\n",
    "    print(f\"ì˜¤ë¥˜: í•´ë‹¹ í´ë” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n{TARGET_PATH}\")\n",
    "else:\n",
    "    pkl_files = [f for f in os.listdir(TARGET_PATH) if f.endswith('.pkl')]\n",
    "    \n",
    "    print(f\"ë¶„ì„ ì‹œì‘: ì´ {len(pkl_files)}ê°œì˜ íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for filename in pkl_files:\n",
    "        file_path = os.path.join(TARGET_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            \n",
    "            # (1) í˜„ì¬ íŒŒì¼ì˜ í–‰ ê°œìˆ˜ ë”í•˜ê¸°\n",
    "            current_count = len(df)\n",
    "            total_blog_count += current_count\n",
    "            \n",
    "            # (2) í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ ê°œìˆ˜ í™•ì¸\n",
    "            # 'ë³¸ë¬¸' ì»¬ëŸ¼ì— \"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨\"ë¼ëŠ” ë¬¸ìì—´ì´ í¬í•¨ëœ í–‰ì˜ ê°œìˆ˜ - NaN ê°’ì€ ì œì™¸í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì²´í¬\n",
    "            if 'ë³¸ë¬¸' in df.columns:\n",
    "                fails = df[df['ë³¸ë¬¸'].astype(str).str.contains(\"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨\", na=False)]\n",
    "                current_fails = len(fails)\n",
    "                total_fail_count += current_fails\n",
    "            else:\n",
    "                current_fails = 0\n",
    "\n",
    "            # (3) ë‚ ì§œ ë¯¸ìƒ ê°œìˆ˜ í™•ì¸\n",
    "            # 'ì‘ì„±ì¼' ì»¬ëŸ¼ì´ \"ë‚ ì§œ ë¯¸ìƒ\"ì¸ í–‰ì˜ ê°œìˆ˜\n",
    "            if 'ì‘ì„±ì¼' in df.columns:\n",
    "                unknowns = df[df['ì‘ì„±ì¼'] == \"ë‚ ì§œ ë¯¸ìƒ\"]\n",
    "                current_unknowns = len(unknowns)\n",
    "                total_unknown_date += current_unknowns\n",
    "            else:\n",
    "                current_unknowns = 0\n",
    "            \n",
    "            print(f\"[{filename}] ì „ì²´: {current_count} | ì‹¤íŒ¨: {current_fails} | ë‚ ì§œë¯¸ìƒ: {current_unknowns}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{filename}] ì½ê¸° ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[ ìµœì¢… ë¶„ì„ ê²°ê³¼ ]\")\n",
    "    print(f\"1. ì´ ìˆ˜ì§‘ ë¸”ë¡œê·¸ ê°œìˆ˜      : {total_blog_count:,} ê°œ\")\n",
    "    print(f\"2. í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (1+2ë‹¨ê³„) : {total_fail_count:,} ê°œ\")\n",
    "    print(f\"3. ë‚ ì§œ ë¯¸ìƒ ê°œìˆ˜           : {total_unknown_date:,} ê°œ\")\n",
    "    \n",
    "    # ì„±ê³µë¥  ê³„ì‚° (ì´ ê°œìˆ˜ - ì‹¤íŒ¨ ê°œìˆ˜) / ì´ ê°œìˆ˜\n",
    "    if total_blog_count > 0:\n",
    "        success_rate = ((total_blog_count - total_fail_count) / total_blog_count) * 100\n",
    "        print(f\"-> ìš”ì²­ ì„±ê³µë¥                : {success_rate:.2f}%\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a3ec0-1359-4895-8312-b31f3a33e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ ì œëŒ€ë¡œ ì•ˆëœ ë°ì´í„° ì œê±°\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "# ë¶„ì„í•  í´ë” ê²½ë¡œ - 3ë²ˆ ì‘ì—… ê²°ê³¼\n",
    "INPUT_PATH = r'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/collected_main_text_blog'\n",
    "\n",
    "# ì •ì œëœ ë°ì´í„° ì €ì¥ìš© í´ë”\n",
    "OUTPUT_PATH = r\"E:ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/blog_cleaned_data\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. í´ë” ìƒì„± ë° ì‘ì—… ì‹œì‘\n",
    "# --------------------------------------------------------------------------\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(f\"ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {OUTPUT_PATH}\")\n",
    "\n",
    "pkl_files = [f for f in os.listdir(INPUT_PATH) if f.endswith('.pkl')]\n",
    "print(f\"ì´ {len(pkl_files)}ê°œì˜ íŒŒì¼ì— ëŒ€í•´ ì •ì œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_removed = 0\n",
    "\n",
    "for filename in pkl_files:\n",
    "    source_path = os.path.join(INPUT_PATH, filename)\n",
    "    save_path = os.path.join(OUTPUT_PATH, filename)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(source_path)\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # [ì •ì œ ë¡œì§] ì‹¤íŒ¨ ë° ë‚ ì§œ ë¯¸ìƒ ì œê±°\n",
    "        # ------------------------------------------------------------------\n",
    "        \n",
    "        # 1. 'ë³¸ë¬¸'ì— 'í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨'ê°€ í¬í•¨ëœ í–‰ ì œê±° - NaN ê°’ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ astype(str) ì‚¬ìš©\n",
    "        condition_fail = df['ë³¸ë¬¸'].astype(str).str.contains(\"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨\")\n",
    "        \n",
    "        # 2. 'ì‘ì„±ì¼'ì´ 'ë‚ ì§œ ë¯¸ìƒ'ì¸ í–‰ ì œê±° (!=)\n",
    "        condition_unknown = (df['ì‘ì„±ì¼'] == \"ë‚ ì§œ ë¯¸ìƒ\")\n",
    "        \n",
    "        # ë‘ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ ì œê±° ëŒ€ìƒ -> ì¦‰, ë‘˜ ë‹¤ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ë‚¨ê¹€\n",
    "        df_cleaned = df[~condition_fail & ~condition_unknown].copy()\n",
    "        \n",
    "        df_cleaned.reset_index(drop=True, inplace=True)      \n",
    "      \n",
    "        # ì œê±°ëœ ê°œìˆ˜ ê³„ì‚°\n",
    "        removed_count = original_count - len(df_cleaned)\n",
    "        total_removed += removed_count\n",
    "        \n",
    "        # ì •ì œëœ íŒŒì¼ ì €ì¥\n",
    "        df_cleaned.to_pickle(save_path)\n",
    "        \n",
    "        print(f\"[{filename}] ì™„ë£Œ | ê¸°ì¡´: {original_count} -> ì •ì œí›„: {len(df_cleaned)} (ì‚­ì œ: {removed_count})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{filename}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ì´ ì‚­ì œëœ í–‰(ì‹¤íŒ¨+ë‚ ì§œë¯¸ìƒ): {total_removed} ê°œ\")\n",
    "print(f\"ì •ì œëœ íŒŒì¼ì€ '{OUTPUT_PATH}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6262669-127c-473d-8b0c-621f3bdb2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ì œ í›„ íŒŒì¼ ì¬í™•ì¸\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. ë¶„ì„í•  í´ë” ê²½ë¡œ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "TARGET_PATH = r\"E:ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/blog_cleaned_data\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "# --------------------------------------------------------------------------\n",
    "total_blog_count = 0      # 1. ì´ ìˆ˜ì§‘ ë¸”ë¡œê·¸ ê°œìˆ˜\n",
    "total_fail_count = 0      # 2. í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ ê°œìˆ˜ (1, 2ë‹¨ê³„ í¬í•¨)\n",
    "total_unknown_date = 0    # 3. ë‚ ì§œ ë¯¸ìƒ ê°œìˆ˜\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. íŒŒì¼ ìˆœíšŒ ë° ë¶„ì„\n",
    "# --------------------------------------------------------------------------\n",
    "if not os.path.exists(TARGET_PATH):\n",
    "    print(f\"ì˜¤ë¥˜: í•´ë‹¹ í´ë” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n{TARGET_PATH}\")\n",
    "else:\n",
    "    pkl_files = [f for f in os.listdir(TARGET_PATH) if f.endswith('.pkl')]\n",
    "    \n",
    "    print(f\"ë¶„ì„ ì‹œì‘: ì´ {len(pkl_files)}ê°œì˜ íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for filename in pkl_files:\n",
    "        file_path = os.path.join(TARGET_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            \n",
    "            current_count = len(df)\n",
    "            total_blog_count += current_count\n",
    "            \n",
    "            if 'ë³¸ë¬¸' in df.columns:\n",
    "                fails = df[df['ë³¸ë¬¸'].astype(str).str.contains(\"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨\", na=False)]\n",
    "                current_fails = len(fails)\n",
    "                total_fail_count += current_fails\n",
    "            else:\n",
    "                current_fails = 0\n",
    "\n",
    "            if 'ì‘ì„±ì¼' in df.columns:\n",
    "                unknowns = df[df['ì‘ì„±ì¼'] == \"ë‚ ì§œ ë¯¸ìƒ\"]\n",
    "                current_unknowns = len(unknowns)\n",
    "                total_unknown_date += current_unknowns\n",
    "            else:\n",
    "                current_unknowns = 0\n",
    "            \n",
    "            print(f\"[{filename}] ì „ì²´: {current_count} | ì‹¤íŒ¨: {current_fails} | ë‚ ì§œë¯¸ìƒ: {current_unknowns}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{filename}] ì½ê¸° ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"[ ìµœì¢… ë¶„ì„ ê²°ê³¼ ]\")\n",
    "    print(f\"1. ì´ ìˆ˜ì§‘ ë¸”ë¡œê·¸ ê°œìˆ˜      : {total_blog_count:,} ê°œ\")\n",
    "    print(f\"2. í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (1+2ë‹¨ê³„) : {total_fail_count:,} ê°œ\")\n",
    "    print(f\"3. ë‚ ì§œ ë¯¸ìƒ ê°œìˆ˜           : {total_unknown_date:,} ê°œ\")\n",
    "    \n",
    "    # ì„±ê³µë¥  ê³„ì‚° (ì´ ê°œìˆ˜ - ì‹¤íŒ¨ ê°œìˆ˜) / ì´ ê°œìˆ˜\n",
    "    if total_blog_count > 0:\n",
    "        success_rate = ((total_blog_count - total_fail_count) / total_blog_count) * 100\n",
    "        print(f\"-> ìš”ì²­ ì„±ê³µë¥                : {success_rate:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. ì‹¤ì œ ì¶œë ¥í•´ ë³¼ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "file_to_check = 'E:ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/blog_cleaned_data/crawled_filtered_combined_1_ì¹´ì¹´ì˜¤í†¡_ì—…ë°ì´íŠ¸.pkl'\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# --------------------------------------------------------------------------\n",
    "try:\n",
    "    df_check = pd.read_pickle(file_to_check)\n",
    "\n",
    "    print(\"íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"[ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ]\")\n",
    "    df_check.info()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"[ ìƒìœ„ 5ê°œ ë°ì´í„° ]\")\n",
    "    display(df_check.head())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 7. ë³¸ë¬¸ ë‚´ìš© ìƒì„¸ í™•ì¸\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"[ ì²« ë²ˆì§¸ í–‰ì˜ ë³¸ë¬¸ ë‚´ìš© ì „ë¬¸ ]\")\n",
    "    print(df_check.loc[0, 'ë³¸ë¬¸'])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\\nê²½ë¡œ: {file_to_check}\")\n",
    "except Exception as e:\n",
    "    print(f\"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e706b6-04c3-4ebd-9c27-4de49f0aa412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. íŒŒì¼ë³„ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—… ì§„í–‰\n",
    "- ëª©í‘œ : ìì—°ì–´ ë¶„ì„ì„ ìœ„í•´ì„œ ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ <b>'ì œëª©ê³¼ ë³¸ë¬¸'</b>ì—ì„œ íŠ¹ë¬¸, ì´ëª¨í‹°ì½˜, ììŒê³¼ ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ì œì™¸í•˜ëŠ” ì „ì²˜ë¦¬ ì‘ì—… ì§„í–‰í•©ë‹ˆë‹¤. \n",
    "- í•µì‹¬ : ìˆ«ìì™€ ì˜ë¬¸ì€ ì œê±°ë˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d9d90-8fd6-4169-a069-da27e8ca94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ëª¨ì§€ ì œê±°ë¥¼ ìœ„í•´ì„œ ì¸ìŠ¤í†¨\n",
    "!pip install emoji\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# ìì—°ì–´ ë¶„ì„ì„ ìœ„í•œ í•¨ìˆ˜ ë§Œë“¤ê¸° & í…ŒìŠ¤íŠ¸\n",
    "def clean_text_with_lib(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. [ìˆ˜ì •ë¨] emoji ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì´ëª¨í‹°ì½˜ì„ ì°¾ì•„ ë¹ˆ ë¬¸ìì—´ë¡œ ë°”ê¾¸ê¸°\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 2. í•œê¸€, ì˜ì–´, ìˆ«ìë¥¼ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'[^A-Za-z0-9ê°€-í£ã„±-ã…ã…-ã…£\\s]', '', text)\n",
    "    \n",
    "    # 3. ììŒ ë˜ëŠ” ëª¨ìŒë§Œ ë°˜ë³µë˜ëŠ” ê²½ìš° ì œê±° (ã…‹ã…‹ã…‹, ã…œã…œã…œ)\n",
    "    text = re.sub(r'([ã„±-ã…ã…-ã…£])\\1{2,}', '', text)\n",
    "    \n",
    "    # 4. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë³€ê²½\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 5. ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ -> ê²°ê³¼ í™•ì¸ í›„ ë°˜ì˜\n",
    "test_review1 = \"ì—…ë°ì´íŠ¸ ìµœì•… 1ì  ë“œë¦½ë‹ˆë‹¤.ã… ã… ã…  UI ëŒë ¤ë†”!!!! #ì¹´ì¹´ì˜¤\"\n",
    "test_review2 = \"ã…‹ã…‹ã…‹ Only salvation is Jesus, believe in Jesus ğŸ™\"\n",
    "test_review3 = \"ì´ëª¨í‹°ì½˜ğŸ˜„ì€ ì¢‹ì€ë°, ì•± ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ìš”...\"\n",
    "test_review4 = \"0ì ì´ ì—†ì–´ì„œ 1ì ë‚¨ê¹€ã…‹ã…‹\"\n",
    "print(f\"ì›ë³¸: {test_review1} -> ì •ì œ í›„: {clean_text_with_lib(test_review1)}\")\n",
    "print(f\"ì›ë³¸: {test_review2} -> ì •ì œ í›„: {clean_text_with_lib(test_review2)}\")\n",
    "print(f\"ì›ë³¸: {test_review3} -> ì •ì œ í›„: {clean_text_with_lib(test_review3)}\")\n",
    "print(f\"ì›ë³¸: {test_review4} -> ì •ì œ í›„: {clean_text_with_lib(test_review4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191980c-ccb0-40c3-a37b-781119a3ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4ë²ˆ ê²°ê³¼ë¡œ íšë“í•œ .pkl íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ\n",
    "INPUT_PATH = \"E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/blog_cleaned_data\"\n",
    "\n",
    "# ì •ì œ ì™„ë£Œëœ .pkl íŒŒì¼ì„ ì €ì¥í•  í´ë” ê²½ë¡œ ì§€ì •\n",
    "OUTPUT_PATH = \"E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/cleaned_main_text_blog\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œë¥¼ ì ìš©í•  ì›ë³¸ ì»¬ëŸ¼ëª… - ë³¸ë¬¸ / ì œëª© ëª¨ë‘ ì ìš© í•„ìš”\n",
    "SOURCE_COLUMN = 'ë³¸ë¬¸'\n",
    "\n",
    "# ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ìƒˆë¡œìš´ ì»¬ëŸ¼ëª… - ë³¸ë¬¸_ì •ì œ / ì œëª©_ì •ì œ\n",
    "TARGET_COLUMN = 'ë³¸ë¬¸_ì •ì œ'\n",
    "\n",
    "# ì‹¤ ë°ì´í„°ì— ë°˜ì˜í•˜ê¸°\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ì¶œë ¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(f\"'{OUTPUT_PATH}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì…ë ¥ í´ë”ì— ìˆëŠ” ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "file_list = os.listdir(INPUT_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"í´ë” ë‚´ ëª¨ë“  .pkl íŒŒì¼ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì •ì œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(f\"ëŒ€ìƒ í´ë”: {INPUT_PATH}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for filename in file_list:\n",
    "    # .pkl íŒŒì¼ë§Œ ëŒ€ìƒ\n",
    "    if filename.endswith('.pkl'):\n",
    "        # íŒŒì¼ ê²½ë¡œ ì¡°í•©\n",
    "        input_filepath = os.path.join(INPUT_PATH, filename)\n",
    "        \n",
    "        print(f\"[{filename}] íŒŒì¼ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_pickle(input_filepath)\n",
    "            \n",
    "            if SOURCE_COLUMN not in df.columns:\n",
    "                print(f\" -> ê²½ê³ : '{SOURCE_COLUMN}' ì»¬ëŸ¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                continue\n",
    "\n",
    "            # í•¨ìˆ˜ ì ìš© í›„ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
    "            df[TARGET_COLUMN] = df[SOURCE_COLUMN].apply(clean_text_with_lib)\n",
    "            \n",
    "            # ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ pkl íŒŒì¼ë¡œ ì €ì¥\n",
    "            output_filename = f\"cleaned_{filename}\"\n",
    "            output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
    "            df.to_pickle(output_filepath)\n",
    "            \n",
    "            print(f\" -> ì™„ë£Œ! '{output_filename}'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" -> ì˜¤ë¥˜ ë°œìƒ: {filename} ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}\\n\")\n",
    "\n",
    "print(\"ëª¨ë“  í…ìŠ¤íŠ¸ ì •ì œ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70e6a4-3aad-45df-b2c4-f77e004c8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½ í›„ íŒŒì¼ ì²´í¬\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "file_to_check = 'E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/cleaned_main_text_blog/cleaned_crawled_filtered_combined_1_ì¹´ì¹´ì˜¤í†¡_ì—…ë°ì´íŠ¸.pkl'\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# --------------------------------------------------------------------------\n",
    "try:\n",
    "    df_check = pd.read_pickle(file_to_check)\n",
    "\n",
    "    print(\"íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # ë°ì´í„°ì˜ ì „ì²´ì ì¸ êµ¬ì¡° í™•ì¸ (í–‰/ì—´ ê°œìˆ˜, ì»¬ëŸ¼ë³„ íƒ€ì… ë“±)\n",
    "    print(\"[ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ]\")\n",
    "    df_check.info()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # ìƒìœ„ 5ê°œ ë°ì´í„° í™•ì¸\n",
    "    print(\"[ ìƒìœ„ 10ê°œ ë°ì´í„° ]\")\n",
    "    display(df_check.head(10))\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 3. ë³¸ë¬¸ ë‚´ìš© ìƒì„¸ í™•ì¸\n",
    "    # --------------------------------------------------------------------------\n",
    "    # ì²« ë²ˆì§¸ í–‰ì˜ 'ë³¸ë¬¸' ë‚´ìš©ì„ ì‹¤ì œë¡œ ì¶œë ¥í•´ë³´ê¸°\n",
    "    print(\"[ ì²« ë²ˆì§¸ í–‰ì˜ ë³¸ë¬¸ ë‚´ìš© ì „ë¬¸ ]\")\n",
    "    print(df_check.loc[1, 'ë³¸ë¬¸'])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\\nê²½ë¡œ: {file_to_check}\")\n",
    "except Exception as e:\n",
    "    print(f\"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d5468-3c5e-4b18-97e0-229da3fbcc56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. ìµœì¢… íŒŒì¼ í†µí•©\n",
    "- ëª©í‘œ : í‚¤ì›Œë“œë³„ë¡œ ìƒì„±ëœ pkl íŒŒì¼ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ìµœì¢… í†µí•©í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db487403-c86a-43b0-a18d-7c7b40695db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "# 5ë²ˆ ê²°ê³¼ íŒŒì¼\n",
    "CLEANED_DATA_PATH = \"E:/ë°ì¼ë¦¬í”„ë¡œì íŠ¸/251120_ì¹´ì¹´ì˜¤í†¡ ë„¤ì´ë²„ê¸°ì‚¬,ë¸”ë¡œê·¸/cleaned_main_text_blog\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. ëª¨ë“  íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (ì „ì²´ í†µê³„ë¥¼ ìœ„í•´)\n",
    "# --------------------------------------------------------------------------\n",
    "# í´ë” ë‚´ì˜ ëª¨ë“  ì •ì œëœ pkl íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°\n",
    "all_cleaned_files = [os.path.join(CLEANED_DATA_PATH, f) for f in os.listdir(CLEANED_DATA_PATH) if f.endswith('.pkl')]\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ ì½ì–´ì™€ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©\n",
    "df_list = [pd.read_pickle(file) for file in all_cleaned_files]\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"ëª¨ë“  ì •ì œëœ ë¸”ë¡œê·¸ íŒŒì¼ í†µí•© ì™„ë£Œ!\")\n",
    "print(f\" -> ì´ ë¸”ë¡œê·¸ê¸€ ìˆ˜: {len(combined_df)} ê°œ\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "combined_df.to_pickle(\"final_blog_all_articles.pkl\")\n",
    "print(\"\\në¸”ë¡œê·¸ íŒŒì¼ í†µí•© ë°ì´í„°ë¥¼ 'final_blog_all_articles.pkl'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d970a-7df4-4eb3-8490-4b0249246e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
