{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Gp3FfSR5L7JY","YB_fSByIJbJZ"],"authorship_tag":"ABX9TyN0T5fixv4+cHfZY9PQvk8Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 네이버 뉴스, 블로그 Url - 크롤링 수집 작업 과정\n","네이버 뉴스와 블로그 Url 수집 작업은 검색량의 제한이 설정되어 있어 매일 각 키워드에 해당하는 Url을 수집한 뒤, 분석 시작시 수집된 일자별 Url의 본문 내용을 수집하는 순서로 진행하였습니다.\n","\n","1. 네이버 뉴스 검색 키워드\n","- 카카오톡 개편, 카카오톡 업데이트, 카톡, 네이트온, 카카오\n","\n","2. 네이버 블로그 검색 키워드\n","- 카카오톡 업데이트, 카카오톡 개편, 카톡 업데이트, 카톡 개편, 네이트온, 라인\n","- 11월 9일 추가 : 카카오톡, 카톡\n","\n","*[!] 네이버 뉴스와 블로그는 매주 목요일(+α)을 기준으로 크롤링에 사용되는 조건값들을 갱신되기 때문에 코드 조정이 필요하였습니다.*\n","\n","*[!] 코렙에서 크롤링 진행 중 Chrome 버전이 맞지 않아 크롤링이 진행되지 않는 호환성 문제가 발생되어 매일 크롤링을 진행할 때 Chrome을 새로 다운로드 받아 설치하는 것으로 문제를 해결하였습니다.*"],"metadata":{"id":"80AuIjcFDVKo"}},{"cell_type":"code","source":["# 셀레니움 사용을 위한 설정 작업(공통)\n","\n","# 1. 셀레니움 라이브러리를 설치합니다.\n","!pip install selenium\n","\n","# 2. 코랩의 리눅스 환경에 크롬 브라우저와 웹 드라이버 설치\n","!apt-get update\n","\n","# 3. 최신 Google Chrome 브라우저를 다운로드하고 설치 - 호환성 문제 해결용\n","!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n","!apt install --yes ./google-chrome-stable_current_amd64.deb\n","\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import TimeoutException\n","from selenium.common.exceptions import TimeoutException, NoSuchElementException\n","\n","# 4. 셀레니움이 코랩의 웹 드라이버를 사용할 수 있도록 설정(별도 창 오픈 안되고 처리되도록)\n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless') # << 요기\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","options.add_argument('--disable-gpu')\n","\n","print(\"✅ WebDriver 세션이 성공적으로 생성되었습니다. 크롤링을 시작할 수 있습니다.\")"],"metadata":{"id":"aq0rgv0YP5b7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Phase 1-1] 네이버 뉴스 URL 수집\n","\n","1. 프로젝트 개요\n","\n","해당 파일은 '카카오톡 업데이트 사용자 피드백 분석' 프로젝트의 데이터 수집 단계 중, 네이버 뉴스의 게시물 URL을 수집하는 과정을 담고 있습니다.\n","\n","네이버 뉴스 검색 결과 페이지의 '기사 덩어리(Cluster)'라는 독특하고 복잡한 구조에 대응하기 위해 Selenium 기반의 맞춤형 스크래핑 로직을 구축했습니다.\n","\n","2. 주요 로직 및 문제 해결 과정\n","\n","네이버 뉴스 검색 결과는 하나의 '대표 기사'와 다수의 '관련 뉴스'가 묶인 '기사 덩어리' 형태로 제공됩니다. 모든 관련 기사의 URL을 누락 없이 수집하기 위해 다음과 같은 전략을 적용했습니다.\n","\n","- 동적 페이지 전체 로딩: Selenium을 활용하여 목표한 수의 '기사 덩어리'가 모두 로드될 때까지 페이지 스크롤을 자동 반복했습니다.\n","- '대표/관련' 구조 파싱: 로드된 페이지 전체를 대상으로, 각 '기사 덩어리'를 먼저 찾고, 그 안에서 다시 '대표 기사' 영역과 '관련 뉴스' 영역을 구분하여 각각의 제목, 언론사, URL 정보를 정교하게 추출하는 '2단계 파싱(Two-step Parsing)' 로직을 구현했습니다.\n","- 안정적인 중간 저장: 수백 개의 '기사 덩어리'를 처리하는 과정에서 발생할 수 있는 데이터 유실을 방지하기 위해, 일정 개수(save_interval)의 덩어리를 처리할 때마다 수집된 URL 목록을 CSV 파일에 주기적으로 중간 저장하는 기능을 포함했습니다.\n","- 실행 환경 안정화: 코랩(Colab) 환경의 크롬 드라이버 호환성 문제에 대응하기 위해, 스크립트 실행 시 항상 최신 버전의 크롬을 설치하는 로직을 추가하여, 장시간의 수집 작업이 안정적으로 수행될 수 있는 환경을 구축했습니다.\n","\n","3. 결과물\n","\n","스크립트 내 입력한 키워드별로 '작성자, 날짜, 제목, URL' 정보가 포함된 CSV 파일이 생성됩니다."],"metadata":{"id":"Gp3FfSR5L7JY"}},{"cell_type":"code","source":["import time\n","import pandas as pd\n","import re\n","\n","# 드라이버 경로를 지정 및 실행\n","browser = webdriver.Chrome(options=options)\n","wait = WebDriverWait(browser, 10)\n","\n","# 검색 키워드 입력\n","keyword = \"카카오\"\n","max_articles_to_collect = 200\n","backup_filename = \"251120_naver_news_links_backup1.csv\"\n","collected_news = []\n","processed_urls = set()\n","\n","print(f\"'{keyword}' 키워드로 최대 {max_articles_to_collect}개의 기사 덩어리 수집을 시작합니다. (관련도순)\")\n","\n","url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}\"\n","browser.get(url)\n","time.sleep(2)\n","\n","# 스크롤하여 모든 기사 로드 - 목표 수량의 기사 '덩어리'가 로드될 때까지 스크롤합니다.\n","while True:\n","    try:\n","        # 기사 블록 코드 설정\n","        # UD3s7I8expz3WkwQ004B > IgJMhH4Xrhuw6jctOE_e > KetQP_LKIbcM61v6xsK_ > vs1RfKE1eTzMZ5RqnhIv > o2YzmBxKhEuKOM4uNxFM > OXZTIahLwg7zu2s4BjcD > dnrvM2QEkuzd3V9zdADN\n","        # zc470m0U4oRqtEfZ3YqX > C27ncwLnr9ZhoFd7p9eW > dHVkS02MdHAPCLuLeYE_ > i3viLIysNP0cK6eVSE_t > Lfnc2BS3iprGnUjPmaau > nFxvxHv3aFvb8orT0c7L > tyhUp9y3pNUE2zu1vg4o\n","        article_clusters = browser.find_elements(By.XPATH, \"//div[contains(@class, 'shjpbJ1U8dIwWXdtD0kq')]\")\n","\n","        print(f\"스크롤 다운... 현재 로드된 기사 덩어리 수: {len(article_clusters)}개\")\n","\n","        if len(article_clusters) >= max_articles_to_collect:\n","            print(f\"목표 수량({max_articles_to_collect}개) 이상의 기사 덩어리가 로드되어 스크롤을 중단합니다.\")\n","            break\n","\n","        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n","        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","        time.sleep(3)\n","        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n","\n","        if new_height == last_height:\n","            print(\"페이지의 끝에 도달하여 스크롤을 중단합니다.\")\n","            break\n","\n","    except Exception as e:\n","        print(f\"스크롤 중 오류 발생: {e}\")\n","        break\n","\n","# 스크롤 완료 후, 로드된 모든 기사 정보 일괄 수집\n","print(\"\\n\" + \"=\"*50)\n","print(\"스크롤 완료. 로드된 모든 '기사 덩어리'를 분석하여 중간 저장합니다.\")\n","print(\"=\"*50)\n","\n","# 기사 블록 코드를 그대로 입력\n","final_article_clusters = browser.find_elements(By.XPATH, \"//div[contains(@class, 'shjpbJ1U8dIwWXdtD0kq')]\")\n","print(f\"총 {len(final_article_clusters)}개의 '기사 덩어리'를 찾았습니다. 정보 추출을 시작합니다.\")\n","\n","# 중간 저장 설정\n","save_interval = 100\n","is_first_save = True\n","temp_list = [] # 중간 저장을 위한 임시 리스트\n","\n","for i, cluster in enumerate(final_article_clusters):\n","    try:\n","        # 1. 대표 기사 정보 수집(대표 기사의 링크 찾기)\n","        # Kv_vr2pgKJ1YQu5GxPEk > g142CmJWlznnvvbvmk68 > oE0MWYkMadhMOexVagqP > VVZqvAlvnADQu8BVMc2n > r5Erm7sXBDLtcwB4qpy7 > nwfSPmNIlDyRhvW6TUmz > MhHMvACwlVBbjWsd7yi3\n","        # SR2LrlI9g02spd3asYd0 > xaRANlI1WEmTnsgGH3eP > EwZelK7klfw43H0sizCm > FjllvMNpyI4wLwvZ4zbu > sS0WyM73AIK9pz1Yqyqq > moM44hE6Je7O8nL1iBI9 > eRlwIV3EhjKD0fYL3LaQ\n","        main_link_element = cluster.find_element(By.XPATH, \".//a[contains(@class, 'yuu64AGiOBzaFbBUUZbL')]\")\n","        href = main_link_element.get_attribute('href')\n","\n","        if href not in processed_urls:\n","            press_name = cluster.find_element(By.XPATH, \".//span[contains(@class, 'sds-comps-profile-info-title-text')]\").text\n","\n","            # 날짜 정보 수집(대표 기사와 SUB 기사 모두 코드 동일함)\n","            # Ltv7sJRJrOOmpZjSEje_ > wcxswLWlceUCHTW8Wb6J > U1zN1wdZWj0pyvj9oyR0 > FeA7N0BhBgO_D8kbFxJF > PgHY7RzLrrRbaQRE6rd6 > fgTXvYZMrLfEWoFdUCTw > MA8c8CAJTXR5VZfzQIye\n","            # pwnjCGK9f7mL1YDI1uYb > SaoU38edkXdPztmLceJP > PMb1a1DEernbhZd2MxiT > Ma1nB_t9KiMSHtLJMRq5 > _WbCGyUrBcsoRc8OCWZC > FGGxCHMwgwpUxnJbY5q7\n","            date_elements = cluster.find_elements(By.XPATH, \".//div[contains(@class, 'r8Zk0bSoBfwe23GKUAiO')]\")\n","            date_str = date_elements[0].text if date_elements else \"날짜 정보 없음\"\n","\n","            temp_list.append({\n","                \"언론사\": press_name, \"날짜_원문\": date_str, \"제목\": main_link_element.text,\n","                \"URL\": href, \"구분\": \"대표 기사\"\n","            })\n","            processed_urls.add(href)\n","\n","        # 2. Sub 기사 정보 수집\n","        try:\n","            # sub 기사를 감싸는 영역 찾기\n","            # kKg41qrHvplVksYUiHBW > Me1nXk2Sj0arGqU1zX5w > bVTBojHRFWtr0ZbV6C5y > FswuEMo_eVFKY6_b3tld > OF07cvLiENTY1VKtFc5A > EwAFgcAS665NvfKxDROd\n","            # Ei1cZ0jRRr6RYsXhyTPO > yvf5xJP1YqnVwu_4khLV > JQDuwtyvXrFV747dop5H > BzIGBxg89LPpHBcTjqMA > PrUOhta1LIOWOfmEvQmw\n","            sub_links_area = cluster.find_element(By.XPATH, \".//div[contains(@class, 'YZ4O2FJngrWp9COsZhRU')]\")\n","\n","            # sub 기사 한개씩의 영역을 찾기\n","            # qpBnmqdPyZ4VYb3aEgnw > wHeGm6Q2cZzREeeoGAek > pXtzVXJhhzmMSeJWgamo > sNqs4qlOsawpNHKodxwe > o_f72EnDpGiEMgGEWQbn > RgzdKee2MqFRfya9VhXk\n","            # sTfU4DwVahUIvJQ9rT8k > LSwpmnQ0XBvFUIVyu5aw > NduKo_r15nFFL3c3ldcs > Ri9R136Wu9IqTDlzHSpZ > JckXcPhmnCSGElhUB9mQ\n","            sub_articles = sub_links_area.find_elements(By.XPATH, \".//div[contains(@class, 'x8YkymRY1k1U89_l2ofe')]\")\n","\n","            for sub_article in sub_articles:\n","                # sub 기사의 링크 찾기\n","                # sOCNRZynP_svsS9PHNyP > pk_3C5bLJhKIBI3qxn7Q > LQ84J3qaJu0K7yYnc7ei > ro0zKDMMT_0keyhkUqcD > oOo9vZQJs74avFqK6iAQ > rW6cNpojrBPcetkA2xLA\n","                # gE9uY284DgZyTd9TGAfk > PHurNvfBgVciz1x0Xh7G > VqC9n9fwv6dcA8ebYm4l > IVReVs7J6QJhAhvZgk5Q > kBfZiHy29ZtrLZnmujW6\n","                sub_link_element = sub_article.find_element(By.XPATH, \".//a[contains(@class, 'jXFuXTqqS_JwIbn0Gl4Q')]\")\n","                sub_href = sub_link_element.get_attribute('href')\n","\n","                if sub_href not in processed_urls:\n","                    sub_title = sub_link_element.text\n","                    sub_press = sub_article.find_element(By.XPATH, \".//span[contains(@class, 'sds-comps-profile-info-title-text')]\").text\n","                    # 대표 기사에서 찾은 날짜 코드 입력\n","                    sub_date_elements = sub_article.find_elements(By.XPATH, \".//div[contains(@class, 'r8Zk0bSoBfwe23GKUAiO')]\")\n","                    sub_date_str = sub_date_elements[0].text if sub_date_elements else \"날짜 정보 없음\"\n","\n","                    temp_list.append({\n","                        \"언론사\": sub_press, \"날짜_원문\": sub_date_str, \"제목\": sub_title,\n","                        \"URL\": sub_href, \"구분\": \"관련 뉴스\"\n","                    })\n","                    processed_urls.add(sub_href)\n","\n","        except NoSuchElementException:\n","            # 관련 뉴스가 없는 경우 통과\n","            pass\n","\n","    except Exception:\n","        continue\n","\n","    # 중간 저장 로직\n","    # save_interval의 배수가 되거나, 마지막 기사 덩어리일 때 저장\n","    if (i + 1) % save_interval == 0 or (i + 1) == len(final_article_clusters):\n","        if temp_list: # 임시 리스트에 데이터가 있을 경우에만 저장\n","            print(f\"-> 기사 덩어리 {i + 1}개 처리 완료. {len(temp_list)}개의 새 정보를 파일에 중간 저장합니다...\")\n","            temp_df = pd.DataFrame(temp_list)\n","\n","            if is_first_save:\n","                temp_df.to_csv(backup_filename, encoding=\"utf-8-sig\", index=False, mode='w')\n","                is_first_save = False\n","            else:\n","                temp_df.to_csv(backup_filename, encoding=\"utf-8-sig\", index=False, mode='a', header=False)\n","\n","            temp_list = []\n","\n","# --- 최종 결과 처리 ---\n","browser.quit()\n","\n","try:\n","    final_df = pd.read_csv(backup_filename)\n","    print(\"\\n\" + \"=\"*50)\n","    print(f\"총 {len(final_df)}개의 기사 정보(Sub 기사 포함)를 성공적으로 수집했습니다.\")\n","    print(\"=\"*50)\n","    display(final_df)\n","except FileNotFoundError:\n","    print(\"\\n수집된 기사 정보가 없습니다.\")"],"metadata":{"id":"YtUhlS0bTUX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Phase 1-2] 네이버 블로그 URL 수집\n","\n","1. 프로젝트 개요\n","\n","해당 파일은 '카카오톡 업데이트 사용자 피드백 분석' 프로젝트의 데이터 수집 단계 중, 네이버 블로그의 게시물 URL을 수집하는 과정을 담고 있습니다.\n","\n","네이버 검색 결과 페이지의 동적 로딩 특성에 대응하기 위해 Selenium을 활용하여 자동화 스크립트를 구축했습니다.\n","\n","2. 주요 로직 및 문제 해결 과정\n","\n","네이버 블로그 검색 결과 페이지는 사용자가 스크롤을 내릴 때마다 새로운 게시물이 동적으로 로드되는 구조를 가지고 있습니다.\n","\n","안정적인 대량 URL 수집을 위해 다음과 같은 전략을 적용했습니다.\n","\n","- 자동 스크롤링: Selenium의 execute_script를 사용하여, 목표한 수집량에 도달하거나 페이지의 끝에 닿을 때까지 자동으로 스크롤을 반복 실행하여 모든 게시물을 로드합니다.\n","- 중복 방지: 수집 과정에서 동일한 URL이 중복으로 저장되는 것을 방지하기 위해, set() 자료구조를 활용하여 이미 처리된 URL은 건너뛰도록 설계했습니다.\n","- 정확한 타겟팅: 네이버의 주기적인 HTML 구조 변경에 대응하기 위해, XPath의 contains 함수를 사용하여 특정 패턴을 가진 클래스 이름을 유연하게 찾아내는 방식으로 목표 요소를 특정했습니다.\n","- 오류 제어: 스크롤 중 예기치 않은 오류가 발생하더라도 전체 작업이 중단되지 않도록 try-except 구문을 적용하고, 디버깅을 위해 오류 발생 시점의 페이지 소스를 저장하는 로직을 포함했습니다.\n","\n","3. 결과물\n","\n","스크립트 내 입력한 키워드별로 '작성자, 날짜, 제목, URL' 정보가 포함된 CSV 파일이 생성됩니다."],"metadata":{"id":"YB_fSByIJbJZ"}},{"cell_type":"code","source":["import time\n","import pandas as pd\n","\n","# 드라이버 경로를 지정 및 실행\n","browser = webdriver.Chrome(options=options)\n","wait = WebDriverWait(browser, 10)\n","\n","# 검색 키워드 입력\n","keyword = \"카카오톡 업데이트\"\n","max_posts_to_collect = 2000\n","\n","collected_posts = []\n","processed_urls = set()\n","\n","print(f\"'{keyword}' 키워드로 네이버 '블로그' 탭에서| 최대 {max_posts_to_collect}개의 게시물 정보 수집을 시작합니다.\")\n","\n","url = f\"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={keyword}\"\n","\n","browser.get(url)\n","time.sleep(2)\n","\n","# --- 스크롤 Loop ---\n","while len(collected_posts) < max_posts_to_collect:\n","    try:\n","        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n","\n","        # 게시물 블록 코드 설정\n","        wait.until(EC.presence_of_element_located(\n","            # mfeHMC8S51Ze981dhxwE > Orf2UUyw2B80LaIjeSZl > Cp6RskavnKnCD7vKgZWE > NtKCZYlcjvHdeUoASy2I > xYjt3uiECoJ0o6Pj0xOU > Hfu47jvQS6pqdbKB6Rpc > oIxNPKojSTvxvkjdwXVC\n","            # ehKpiBNGSFhS0YAl77ql > UFCFF04iBeRqybsMCKOx > PwscyUqz7vsmI7BTtlKF > Q752ck25tDSrEHVrEwXu > h5EzM_og0Ma9AHxIEpcW > YOhidyWXAbmCpDK3cyOc\n","            (By.XPATH, \"//div[contains(@class, 'VU0scrRNz7gCayloyXne')]\")\n","        ))\n","        # 게시물 블록 코드를 그대로 입력\n","        post_blocks = browser.find_elements(By.XPATH, \"//div[contains(@class, 'VU0scrRNz7gCayloyXne')]\")\n","\n","        print(f\"스크롤 다운... 현재 로드된 게시물 수: {len(post_blocks)}개. (수집 목표: {max_posts_to_collect}개)\")\n","\n","        start_index = len(processed_urls)\n","\n","        newly_loaded_blocks = post_blocks[start_index:]\n","\n","        for item in newly_loaded_blocks:\n","            if len(collected_posts) >= max_posts_to_collect:\n","                break\n","            try:\n","                # 제목/링크 <a> 태그 설정\n","                # Pcw4FFPrGxhURyUmBGxh > M9lOyC5Ckpmk7fKVCMeD > pHHExKwXvRWn4fm5O0Hr > z1n21OFoYx6_tGcWKL_x > CC5p8OBUeZzCymeWTg7v > FRu1e9uGplhm8_kzolkC > VL4Ep4IHjrVh0IEZTTPu\n","                # XAGjLtLX_Jwt7Ucd8crh > Cl492OuZm1euNoyyjv4M > kBKqMEieFEgrAIYJK3vg > vnDTNtnoko3GR0aJilUy > zsOIyFgaikMtT9gmM_tR > wi0982ME6rRJ9toqt_l_\n","                title_element = item.find_element(By.XPATH, \".//a[contains(@class, 'WguH38209auzUF3e7wuS')]\")\n","                href = title_element.get_attribute('href')\n","\n","                if href not in processed_urls:\n","                    title = title_element.text\n","\n","                    # 작성자는 'sds-comps-profile-info-title-text' 클래스를 포함하는 <span> 태그 - 변경 없음\n","                    author = item.find_element(By.XPATH, \".//span[contains(@class, 'sds-comps-profile-info-title-text')]\").text\n","\n","                    # 날짜는 'sds-comps-profile-info-subtext' 클래스를 가진 <span> (기존과 동일) - 변경 없음\n","                    date_str = item.find_element(By.XPATH, \".//span[contains(@class, 'sds-comps-profile-info-subtext')]\").text\n","\n","\n","                    collected_posts.append({\n","                        \"작성자\": author,\n","                        \"날짜\": date_str,\n","                        \"제목\": title_element.text,\n","                        \"URL\": href\n","                    })\n","                    processed_urls.add(href)\n","            except Exception:\n","                continue\n","\n","        if len(collected_posts) >= max_posts_to_collect:\n","            print(f\"목표 수집량({max_posts_to_collect}개)에 도달하여 스크롤을 중단합니다.\")\n","            break\n","\n","        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","        time.sleep(3)\n","        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n","\n","        if new_height == last_height:\n","            print(\"페이지의 끝에 도달하여 스크롤을 중단합니다.\")\n","            break\n","\n","    except Exception as e:\n","        print(f\"스크롤 중 오류 발생: {e}\")\n","        # 오류 발생 시 디버깅 파일 저장\n","        print(\"디버깅을 위해 현재 페이지 소스를 저장합니다.\")\n","        with open('debug_error_page.html', 'w', encoding='utf-8') as f:\n","            f.write(browser.page_source)\n","        break\n","\n","# --- 최종 결과 확인 ---\n","if collected_posts:\n","    df_links = pd.DataFrame(collected_posts)\n","    print(\"\\n\" + \"=\"*50)\n","    print(f\"총 {len(df_links)}개의 게시물 정보를 수집했습니다.\")\n","    print(\"=\"*50)\n","    display(df_links)\n","else:\n","    print(\"\\n수집된 게시물 정보가 없습니다.\")\n","\n","browser.quit()"],"metadata":{"id":"dEvunwxLYx9Q"},"execution_count":null,"outputs":[]}]}