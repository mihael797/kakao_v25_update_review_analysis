{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037fc8cf-8914-474f-b101-0a29b6481bf3",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 데이터 수집 및 전처리 파이프라인\n",
    "\n",
    "네이버 검색을 통해 일자별로 수집된 URL 목록을 이용해 각 언론사로 이동해 기사의 제목과 본문 데이터를 수집하고, 분석에 적합한 형태로 가공하는 전체 과정을 담고 있습니다. 데이터의 정확성과 효율성을 높이기 위해 다음과 같은 체계적인 단계를 거쳐 진행되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e994ff-6f2d-4899-87b9-cbec478cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Selenium 라이브러리 ---\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe2a03-9ce0-4433-b2f1-86ff14bb1bc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. 날짜 폴더에 흩어져 있는 파일 병합(중복 제거 - 링크 기준)\n",
    "- 목표 : 일자별 폴더에 등록되어 있는 '5개 키워드(카카오톡 개편, 카카오톡 업데이트, 카톡, 네이트온, 카카오)'로 검색한 결과 파일들을 URL 기준으로 하나의 데이터 프레임에 통합합니다.\n",
    "- 핵심 : set() 자료구조를 활용하여, 여러 번 수집되었을 수 있는 중복 URL을 이 단계에서 미리 제거하여 데이터의 정합성을 확보하고, 이후의 크롤링 작업 효율을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5f434-0868-43cb-baa0-ae93bf3cc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 일자별로 수집된 검색 결과 파일 저장 경로 지정(load file)\n",
    "BASE_PATH = 'E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/data'\n",
    "# 결과 파일 저장 경로 지정\n",
    "OUTPUT_PATH = 'E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/merged_data_news'\n",
    "\n",
    "# 수집된 파일명(예: '251005_naver_news_links_backup1')에 키워드 대신 숫자를 입력했었기 때문에 실제 어떤 키워드를 의미하는지를 매칭시키기 \n",
    "keyword_map = {\n",
    "    1: '카카오톡_개편',\n",
    "    2: '카카오톡_업데이트',\n",
    "    3: '카톡',\n",
    "    4: '네이트온',\n",
    "    5: '카카오'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 파일 경로 수집 및 키워드별 그룹화\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"파일 검색 시작\")\n",
    "\n",
    "# 키워드 번호(1~5)별로 파일 경로를 저장할 딕셔너리를 생성합니다.\n",
    "file_groups = {num: [] for num in range(1, 6)}\n",
    "\n",
    "# BASE_PATH 아래 날짜 폴더가 있기 때문에 내부 폴더 검색하게 반복문 적용\n",
    "for dir_name in os.listdir(BASE_PATH):\n",
    "    dir_path = os.path.join(BASE_PATH, dir_name)\n",
    "    \n",
    "    if os.path.isdir(dir_path):\n",
    "        # 폴더 내의 파일들을 확인합니다.\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            # 'backup'로 끝나고 '.csv' 확장자를 가진 파일만 대상\n",
    "            if 'backup' in file_name and file_name.endswith('.csv'):\n",
    "                try:\n",
    "                    # 파일명에서 숫자 부분(e.g., 'backup' -> 1) 추출\n",
    "                    file_num = int(file_name.split('backup')[1].split('.csv')[0])\n",
    "                    if file_num in file_groups:\n",
    "                        full_path = os.path.join(dir_path, file_name)\n",
    "                        file_groups[file_num].append(full_path)\n",
    "                except (ValueError, IndexError):\n",
    "                    # 파일명 형식이 예상과 다를 경우 오류 메시지를 출력 후 패스\n",
    "                    print(f\"파일명 형식이 달라 건너뜁니다: {file_name}\")\n",
    "\n",
    "print(\"파일 그룹화가 완료되었습니다.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 그룹별 파일 병합, 중복 제거 및 저장\n",
    "# --------------------------------------------------------------------------\n",
    "# 결과 파일을 저장할 폴더가 없으면 새로 생성\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(f\"'{OUTPUT_PATH}' 폴더를 생성했습니다.\")\n",
    "\n",
    "# 그룹화된 파일들을 병합 작업하기\n",
    "for num, file_list in file_groups.items():\n",
    "    if not file_list:\n",
    "        print(f\"키워드 그룹 {num} ({keyword_map[num]})에 해당하는 파일이 없습니다.\")\n",
    "        continue\n",
    "\n",
    "    # 각 그룹의 모든 CSV 파일을 DataFrame으로 읽어 리스트에 담고 합치기\n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # 'URL' 컬럼을 기준으로 중복된 행 제거(첫번째 파일만 남김)\n",
    "    deduplicated_df = combined_df.drop_duplicates(subset=['URL'], keep='first')\n",
    "    \n",
    "    # 결과 파일을 CSV로 저장(인코딩 꼭 적기!!)\n",
    "    output_filename = f\"combined_{num}_{keyword_map[num]}.csv\"\n",
    "    output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
    "    deduplicated_df.to_csv(output_filepath, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 작업 결과 출력\n",
    "    print(f\"'{output_filename}' 파일 생성 완료!\")\n",
    "    print(f\" - 통합 전 총 데이터 수: {len(combined_df)}개\")\n",
    "    print(f\" - 중복 제거 후 총 데이터 수: {len(deduplicated_df)}개\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"모든 작업이 성공적으로 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aac02c-4bf4-4f9a-bda0-add73dfdf1db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. 병합된 파일에서 중요 키워드를 포함하지 않는 기사 제외하기\n",
    "- 목표 : 분석과 관련 없는 데이터를 제거하여 분석의 정확도를 높입니다.\n",
    "- 핵심 : 제목에 '설정한 핵심 키워드'를 포함하지 않는 뉴스 기사를 본문 수집 전에 필터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a297ed-dc9e-4ec4-bef2-5e4076196676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 1단계 결과 폴더 연결\n",
    "INPUT_PATH = 'E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/merged_data_news'\n",
    "\n",
    "# 결과 파일 저장 경로 지정\n",
    "FILTERED_OUTPUT_PATH = 'E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/filtered_data_news'\n",
    "\n",
    "# '제목' 컬럼에 키워드가 없으면 제거\n",
    "KEYWORDS_TO_CHECK = [\"카카오톡\", \"업데이트\", \"개편\", \"카톡\", \"네이트온\", \"라인\", \"카카오\", \"메신저\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 폴더 생성 및 필터링 작업 시작\n",
    "# --------------------------------------------------------------------------\n",
    "# 결과 파일을 저장할 폴더가 없으면 새로 생성\n",
    "if not os.path.exists(FILTERED_OUTPUT_PATH):\n",
    "    os.makedirs(FILTERED_OUTPUT_PATH)\n",
    "    print(f\"'{FILTERED_OUTPUT_PATH}' 폴더를 생성했습니다.\")\n",
    "\n",
    "print(\"제목 기반 키워드 필터링을 시작합니다...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 파일 처리 및 필터링\n",
    "# --------------------------------------------------------------------------\n",
    "# INPUT_PATH 폴더에 있는 모든 파일(csv 파일만 대상으로) 체크\n",
    "for filename in os.listdir(INPUT_PATH):\n",
    "    # .csv 파일만 대상으로 작업을 수행합니다.\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(INPUT_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            original_rows = len(df)\n",
    "            \n",
    "            # '제목' 컬럼이 있는 지 체크\n",
    "            if '제목' not in df.columns:\n",
    "                print(f\"'{filename}' 파일에 '제목' 컬럼이 없어 건너뜁니다.\")\n",
    "                continue\n",
    "                \n",
    "            # 키워드 리스트를 OR 조건으로 묶어 하나의 검색 패턴으로 생성\n",
    "            search_pattern = '|'.join(KEYWORDS_TO_CHECK)\n",
    "            \n",
    "            # '제목' 컬럼에서 키워드 패턴을 포함하는 모든 행 추출\n",
    "            # na=False 옵션은 '제목'이 비어있는(NaN) 경우 False로 처리하여 오류를 방지함\n",
    "            filtered_df = df[df['제목'].str.contains(search_pattern, na=False)].copy()\n",
    "            \n",
    "            # 필터링 후 남은 데이터의 행 개수 저장\n",
    "            filtered_rows = len(filtered_df)\n",
    "            \n",
    "            # 필터링된 결과를 새로운 파일로 저장\n",
    "            output_filename = f\"filtered_{filename}\"\n",
    "            output_filepath = os.path.join(FILTERED_OUTPUT_PATH, output_filename)\n",
    "            filtered_df.to_csv(output_filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # 작업 결과 출력\n",
    "            print(f\"파일 처리 완료: {filename}\")\n",
    "            print(f\" - 원본 데이터 수: {original_rows}개\")\n",
    "            print(f\" - 필터링 후 데이터 수: {filtered_rows}개\")\n",
    "            print(f\" - 결과 저장 파일: {output_filename}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        except Exception as e:\n",
    "            # 파일 처리 중 문제가 발생하면 오류 메시지를 출력합니다.\n",
    "            print(f\"'{filename}' 파일 처리 중 오류 발생: {e}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "print(\"모든 필터링 작업이 성공적으로 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce9ab5-8b37-4c24-9c3e-0d997016d611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. 뉴스에서 '관련 뉴스'의 언론사명이 '날짜_원문' 컬럼으로 저장되어 수정 작업 진행하기\n",
    "- 목표 : 네이버 뉴스에서 '관련 뉴스'로 검색된 기사의 언론사 컬럼에 sub-article, 날짜 컬럼에 언론사명이 등록되어 각 컬럼에 맞는 데이터가 입력되도록 전처리 작업을 진행합니다. \n",
    "- 핵심 : 데이터가 밀려 입력된 컬럼 값을 재조정하여 언론사명을 기준으로 sorting & grouping 할 수 있도록 전처리 작업을 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dc7e4-159d-4b94-92c0-fd9a71edf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 2단계 결과 폴더 연결\n",
    "INPUT_PATH = \"E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/filtered_data_news\"\n",
    "\n",
    "# 결과 파일 저장 경로 지정\n",
    "OUTPUT_PATH = \"E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/press_cleaned_data_news\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 메인 실행 로직: 파일 순회, 정제, 저장\n",
    "# --------------------------------------------------------------------------\n",
    "# 출력 폴더가 없으면 새로 생성\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(f\"'{OUTPUT_PATH}' 폴더를 생성했습니다.\")\n",
    "\n",
    "# 입력 폴더에 있는 모든 파일 목록 호출\n",
    "file_list = os.listdir(INPUT_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"폴더 내 모든 .csv 파일에 대한 '언론사' 컬럼 정제 작업을 시작합니다.\")\n",
    "print(f\"대상 폴더: {INPUT_PATH}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# 파일 목록을 순회하며 작업을 수행합니다.\n",
    "for filename in file_list:\n",
    "    # .csv 파일만 대상\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(INPUT_PATH, filename)\n",
    "        \n",
    "        print(f\"[{filename}] 파일 처리 중...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(input_filepath)\n",
    "                      \n",
    "            # 2-1. '언론사' 컬럼이 \"Sub-article\"인 행들만 선택\n",
    "            mask = df['언론사'] == 'Sub-article'\n",
    "            \n",
    "            # 2-2. 해당 행이 하나라도 있는지 확인\n",
    "            if mask.sum() > 0:\n",
    "                print(f\" -> 'Sub-article' {mask.sum()}건 발견. 정제를 시작합니다.\")\n",
    "\n",
    "                # 2-2-1. '날짜_원문' 컬럼에서 첫 숫자가 나오기 전까지의 텍스트(언론사명)를 추출\n",
    "                extracted_press = df.loc[mask, '날짜_원문'].str.extract(r'^([^\\d]+)', expand=False)\n",
    "                \n",
    "                # 2-2-2. 추출된 언론사명으로 '언론사' 컬럼을 업데이트\n",
    "                df.loc[mask, '언론사'] = extracted_press\n",
    "            \n",
    "            else:\n",
    "                print(\" -> 'Sub-article'이 없어 통과합니다.\")\n",
    "\n",
    "            # 2-3. 결과를 새로운 파일명으로 변경하여 CSV 파일로 저장\n",
    "            output_filename = f\"press_cleaned_{filename}\"\n",
    "            output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
    "            df.to_csv(output_filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(f\" -> 완료! '{output_filename}'으로 저장되었습니다.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" -> 오류 발생: {filename} 처리 중 문제가 발생했습니다. 오류: {e}\\n\")\n",
    "\n",
    "print(\"모든 '언론사' 컬럼 정제 작업이 성공적으로 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d9388-89ac-4510-bb43-a54f30255de2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. 신문사별 기사를 분리 후 기사 발행량이 많은 순으로 정렬\n",
    "- 목표 : 어떤 신문사의 기사를 수집해야할지 결정하기 위해 신문사별로 기사를 정렬한 뒤, 기사 발행량순으로 정리해서 기준 설정합니다. \n",
    "- 핵심 : 기사수가 많은 신문사부터 크롤링 작업을 진행하고, 너무 기사수가 적은 신문사는 제외합니다.\n",
    "<b> < 기준 설정 > 50개 이상 기사가 있는 언론사부터 먼저 크롤링 진행, 크롤링 작업 시간에 따라 3개 이상 기사가 있는 언론사까지 확장 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5ed69-d3a0-44e5-9c56-c6a4b6ca7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# --------------------------------------------------------------------------\n",
    "# 3단계 결과 폴더 연결\n",
    "CLEANED_DATA_PATH = \"E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/press_cleaned_data_news\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 전체 통계를 위해 모든 파일을 하나로 합치기\n",
    "# --------------------------------------------------------------------------\n",
    "# 폴더 내의 모든 정제된 csv 파일 경로를 리스트 만들기\n",
    "all_cleaned_files = [os.path.join(CLEANED_DATA_PATH, f) for f in os.listdir(CLEANED_DATA_PATH) if f.endswith('.csv')]\n",
    "\n",
    "df_list = [pd.read_csv(file) for file in all_cleaned_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"모든 정제된 뉴스 파일 통합 완료!\")\n",
    "print(f\" -> 총 기사 수: {len(combined_df)} 개\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 언론사별 기사 수 집계 및 출력\n",
    "# --------------------------------------------------------------------------\n",
    "# 언론사 명에 \"\\n\"이 추가된 경우 제거하기 (3번 작업 결과 확인 후 '토큰포스트\\n' 같은 케이스가 있어서)\n",
    "combined_df['언론사'] = combined_df['언론사'].str.strip()\n",
    "\n",
    "press_counts = combined_df['언론사'].value_counts()\n",
    "\n",
    "print(\"[ 언론사별 기사 수 ]\")\n",
    "print(press_counts)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# [!] 기수 수가 몇개 이상인 언론사를 선정할지 - 체크용\n",
    "# --------------------------------------------------------------------------# \n",
    "# 기사 수가 50개 이상인 언론사\n",
    "print(f\"50개 이상: {(press_counts >= 50).sum()} 개\")\n",
    "# 기사 수가 20개 이상인 언론사\n",
    "print(f\"20개 이상: {(press_counts >= 20).sum()} 개\")\n",
    "# 기사 수가 10개 이상인 언론사\n",
    "print(f\"10개 이상: {(press_counts >= 10).sum()} 개\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. 분석 대상이 될 주요 언론사 필터링\n",
    "# --------------------------------------------------------------------------\n",
    "# press_counts에서 기사 수가 50개 이상인 언론사만 선택\n",
    "major_press = press_counts[press_counts >= 50]\n",
    "\n",
    "print(f\"총 {len(press_counts)}개 언론사 중,\")\n",
    "print(f\"기사 수가 50개 이상인 주요 언론사는 총 {len(major_press)}개 입니다.\")\n",
    "print(\"-\" * 30)\n",
    "print(\"[ 주요 언론사 목록 (기사 수 50개 이상) ]\")\n",
    "print(major_press)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. 주요 언론사의 기사만 따로 저장 > 실제 데이터 크롤링 대상 언론사\n",
    "# --------------------------------------------------------------------------\n",
    "major_df = combined_df[combined_df['언론사'].isin(major_press.index)].copy()\n",
    "\n",
    "print(f\"\\n주요 언론사의 총 기사 수: {len(major_df)} 개\")\n",
    "\n",
    "major_df.to_pickle(\"major_press_articles.pkl\")\n",
    "print(\"\\n주요 언론사 기사 데이터를 'major_press_articles.pkl'로 저장했습니다.\")\n",
    "\n",
    "print(major_press.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd739939-00a1-46f6-be59-067b1ef8d00b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. 신문사별 본문 내용 크롤링 전 그룹핑 & 기사 날짜 및 본문 크롤링 작업\n",
    "[ 그룹 1: 통신사 (News Agencies) ] > 코드 포함\n",
    "- 특징: 가장 기본적인 1차 뉴스 생산자.\n",
    "- 목록: 뉴스1, 뉴시스, 연합뉴스\n",
    "\n",
    "[ 그룹 2: 주요 경제지 (Major Economic Dailies) ] > 코드 포함\n",
    "- 특징: 경제/산업 분야에 집중. \n",
    "- 목록: 한국경제, 이데일리, 파이낸셜뉴스, 서울경제, 머니투데이, 매일경제, 헤럴드경제, EBN, 머니S, 이뉴스투데이, 이코노믹리뷰, 브릿지경제, 글로벌이코노믹, 비즈니스포스트, CEO스코어데일리, 파이낸셜포스트\n",
    "\n",
    "[ 그룹 7: 기타 전문지 (Others) ] > 코드 포함\n",
    "- 특징: 위 그룹에 속하지 않는 기타 전문 분야 매체.\n",
    "- 목록: 중앙이코노미뉴스\n",
    "\n",
    "================================ 샘플 코드는 그룹 1, 2, 7만 ========================================\n",
    "\n",
    "[ 그룹 3: IT 전문 매체 (IT/Tech Media) ]\n",
    "- 특징: IT, 테크, 스타트업 전문. \n",
    "- 목록: 뉴스핌, 전자신문, 디지털데일리, 조선비즈, 아이뉴스24, 디지털투데이, 지디넷코리아, 디지털타임스, 테크M, IT조선\n",
    "\n",
    "[ 그룹 4: 주요 종합일간지 (Major General Dailies) ]\n",
    "- 특징: 전통적인 주요 신문사. \n",
    "- 목록: 조선일보, 중앙일보, 세계일보, 국민일보, 동아일보, 서울신문, 메트로신문, 매일일보\n",
    "\n",
    "[ 그룹 5: 방송사 (Broadcasting Companies) ]\n",
    "- 특징: TV 방송을 기반으로 한 언론사. \n",
    "- 목록: YTN, SBS Biz, 연합뉴스TV, SBS, MBN, KBS, 한국경제TV, 서울경제TV\n",
    "\n",
    "[ 그룹 6: 인터넷 종합/연예 매체 (Internet General/Entertainment Media) ]\n",
    "- 특징: 온라인을 기반으로 한 종합/연성 뉴스. \n",
    "- 목록: 데일리안, 이투데이, 아시아경제, 노컷뉴스, 아주경제, 인사이트, 톱스타뉴스, 뉴스웨이, 뉴데일리, 포인트데일리, 천지일보, 핀포인트뉴스, 데일리한국, 서울와이어, 마이데일리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0a9ff-35ef-4e9c-89dc-3b5a6192335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news1(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    \n",
    "    # Selenium 옵션 설정 (브라우저 창이 뜨지 않도록 headless 모드 사용)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('window-size=1920x1080')\n",
    "    options.add_argument(\"disable-gpu\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 페이지 로딩 대기\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(1)\n",
    "        \n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "        \n",
    "        # 날짜 수집\n",
    "        meta_pub = soup.find('meta', property='article:published_time')\n",
    "        meta_mod = soup.find('meta', property='article:modified_time')\n",
    "        time_pub = soup.find('time', id='published')\n",
    "        time_upd = soup.find('time', id='updated') # 추가된 부분\n",
    "\n",
    "        if meta_pub:\n",
    "            article_date = meta_pub['content']\n",
    "        elif meta_mod:\n",
    "            article_date = meta_mod['content']\n",
    "        elif time_pub and 'datetime' in time_pub.attrs:\n",
    "            article_date = time_pub['datetime']\n",
    "        elif time_upd and 'datetime' in time_upd.attrs:\n",
    "            article_date = time_upd['datetime']\n",
    "        else:\n",
    "            article_date = \"날짜 태그를 찾을 수 없음\"\n",
    "\n",
    "        # 본문 수집\n",
    "        article_area = soup.find('article', class_='col-lg-7')\n",
    "        if article_area:\n",
    "            article_body = article_area.find('div', id='articleBodyContent')\n",
    "            if article_body:\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        if content_text == \"본문 수집 실패\":\n",
    "             article_body_backup = soup.find('div', id='articleBodyContent')\n",
    "             if article_body_backup:\n",
    "                 content_text = article_body_backup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text = f\"오류: {e}\"\n",
    "        article_date = f\"오류: {e}\"\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_newsis(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('article')\n",
    "            if article_body:\n",
    "                for script in article_body.find_all(['script', 'iframe', 'div'], class_=['summury', 'view_ad']):\n",
    "                    script.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            info_line = soup.find('div', class_='infoLine')\n",
    "            if info_line:\n",
    "                date_text = info_line.find('p', class_='txt').get_text(strip=True)\n",
    "                # \"등록 2025.11.21 06:00:00\"에서 날짜/시간 부분만 추출 (정규표현식 사용)\n",
    "                match = re.search(r'(\\d{4}\\.\\d{2}\\.\\d{2}\\s\\d{2}:\\d{2}:\\d{2})', date_text)\n",
    "                if match:\n",
    "                    article_date = match.group(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_yonhapnews(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            article_body = soup.find('div', class_='story-news article')\n",
    "            if article_body:\n",
    "                # 1. 저작권 정보를 담고 있는 마지막 p태그 제외\n",
    "                paragraphs = article_body.find_all('p', class_=lambda c: c != 'txt-copyright')\n",
    "                \n",
    "                # 2. 각 p태그의 텍스트 리스트에 담기\n",
    "                content_list = []\n",
    "                for p in paragraphs:\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text: # 내용이 있는 텍스트만 추가\n",
    "                        content_list.append(text)\n",
    "                \n",
    "                # 3. 리스트의 모든 텍스트를 줄바꿈 문자로 연결\n",
    "                content_text = '\\n'.join(content_list)\n",
    "            else:\n",
    "                content_text = \"본문 영역(div.story-news.article) 찾기 실패\"\n",
    "\n",
    "            # 날짜 수집 - 날짜가 기사 본문 바깥에 있어서 전체 soup에서 다시 찾음\n",
    "            date_tag = soup.select_one('p.txt-copyright .date .txt01')\n",
    "            if date_tag:\n",
    "                article_date = date_tag.get_text(strip=True).replace('송고', '').strip()\n",
    "            else:\n",
    "                # 날짜를 못 찾을 경우를 대비한 대체 경로\n",
    "                date_tag = soup.select_one('.share-info .p-info .tit-txt')\n",
    "                if date_tag:\n",
    "                     article_date = date_tag.get_text(strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_fnnews(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('div', id='article_content')\n",
    "            if article_body:\n",
    "                # 불필요한 영역 제거\n",
    "                for tag in article_body.find_all(['div', 'p'], class_=['view_img', 'view_copyright', 'etr-keyword']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            date_area = soup.find('span', class_='row-2')\n",
    "            if date_area:\n",
    "                date_tag = date_area.find('p', text=re.compile('입력'))\n",
    "                if date_tag:\n",
    "                    # \"입력 2025.11.19 07:23\"에서 날짜/시간 부분만 추출\n",
    "                    match = re.search(r'(\\d{4}\\.\\d{2}\\.\\d{2}\\s\\d{2}:\\d{2})', date_tag.get_text())\n",
    "                    if match:\n",
    "                        article_date = match.group(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_edaily(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('div', class_='news_body')\n",
    "            if article_body:\n",
    "                # 불필요한 광고/추천 영역 제거\n",
    "                for tag in article_body.find_all(['table', 'div'], class_=['gg_textshow', 'view_ad01', 'view_ad02']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            date_area = soup.find('div', class_='dates')\n",
    "            if date_area:\n",
    "                date_tag = date_area.find('p', text=re.compile('등록'))\n",
    "                if date_tag:\n",
    "                    # \"등록 2025-11-17 오후 1:53:26\"에서 날짜/시간 부분 추출\n",
    "                    # '오후/오전'을 처리하고, 날짜 형식을 표준에 맞게 변환\n",
    "                    date_text = date_tag.get_text()\n",
    "                    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})\\s(오전|오후)\\s(\\d{1,2}:\\d{2}:\\d{2})', date_text)\n",
    "                    if match:\n",
    "                        date_part, am_pm, time_part = match.groups()\n",
    "                        # 오후 시간 처리 (12시간제 -> 24시간제)\n",
    "                        hour = int(time_part.split(':')[0])\n",
    "                        if am_pm == '오후' and hour != 12:\n",
    "                            hour += 12\n",
    "                        elif am_pm == '오전' and hour == 12: # 오전 12시는 0시\n",
    "                            hour = 0\n",
    "                        \n",
    "                        time_part_24h = f\"{hour:02d}:{time_part.split(':')[1]}:{time_part.split(':')[2]}\"\n",
    "                        article_date = f\"{date_part} {time_part_24h}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_heraldcorp(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 본문 영역(id='articleText')이 나타날 때까지 최대 10초 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleText\"))\n",
    "        )\n",
    "        \n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "        \n",
    "        # 본문 수집\n",
    "        article_body = soup.find('article', id='articleText')\n",
    "        if article_body:\n",
    "            for tag in article_body.find_all(['div', 'figure']):\n",
    "                tag.decompose()\n",
    "            content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # 날짜 수집\n",
    "        date_tag = soup.find('p', class_='date')\n",
    "        if date_tag:\n",
    "            date_text = date_tag.get_text()\n",
    "            match = re.search(r'입력\\s*(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2})', date_text)\n",
    "            if match:\n",
    "                article_date = match.group(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"Selenium 오류: {e}\", f\"Selenium 오류: {e}\"\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_sedaily(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('div', class_='article_view')\n",
    "            if article_body:\n",
    "                related_news = article_body.find('div', class_='art_rel')\n",
    "                if related_news:\n",
    "                    related_news.decompose()\n",
    "                for tag in article_body.find_all(['figure', 'span'], class_=['link_figure', 'pop_link']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            date_area = soup.find('div', class_='article_info')\n",
    "            if date_area:\n",
    "                # article_info 영역의 모든 텍스트를 가져옴\n",
    "                date_text = date_area.get_text(separator='\\n', strip=True)\n",
    "                # 정규 표현식으로 날짜/시간 패턴을 찾음\n",
    "                match = re.search(r'(\\d{4}\\.\\d{2}\\.\\d{2}\\s\\d{2}:\\d{2}:\\d{2})', date_text)\n",
    "                if match:\n",
    "                    article_date = match.group(1).replace('.', '-', 2).replace('.', ' ') # 형식 변경\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_mt(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 1. 날짜 수집\n",
    "            date_tag = soup.find('span', class_='date')\n",
    "            if date_tag:\n",
    "                article_date = date_tag.get_text(strip=True)\n",
    "            else:\n",
    "                article_date = \"날짜 태그(span.date) 찾기 실패\"\n",
    "\n",
    "            # 2. 본문 수집\n",
    "            article_body = soup.find('div', id='articleView')\n",
    "            if article_body:\n",
    "                # articleView 내부의 모든 <p> 태그를 찾음\n",
    "                paragraphs = article_body.find_all('p')\n",
    "                \n",
    "                content_list = []\n",
    "                for p in paragraphs:\n",
    "                    # p 태그에서 텍스트를 추출하고, 내용이 있는 경우에만 리스트에 추가\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text:\n",
    "                        content_list.append(text)\n",
    "                \n",
    "                # 리스트의 텍스트들을 줄바꿈 문자로 합쳐서 최종 본문을 만듦\n",
    "                content_text = '\\n'.join(content_list)\n",
    "            else:\n",
    "                content_text = \"본문 영역(div#articleView) 찾기 실패\"\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_mk(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('div', class_='news_cnt_detail_wrap')\n",
    "            if article_body:\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            date_tag = soup.find('dl', class_='registration').find('dd')\n",
    "            if date_tag:\n",
    "                article_date = date_tag.get_text(strip=True)\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_ebn(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('article', id='article-view-content-div')\n",
    "            if article_body:\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            date_area = soup.find('ul', class_='article-info no-bullet')\n",
    "            if date_area:\n",
    "                date_tag = date_area.find('li', text=re.compile('입력'))\n",
    "                if date_tag:\n",
    "                    date_text = date_tag.get_text(strip=True).replace(\"입력\", \"\")\n",
    "                    article_date = date_text\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_hankyung(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('div', id='articletxt')\n",
    "            if article_body:\n",
    "                # 불필요한 이미지, 광고 영역 제거\n",
    "                for tag in article_body.find_all(['figure', 'div'], class_=['article-figure', 'ad-area-wrap']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            date_area = soup.find('div', class_='datetime')\n",
    "            if date_area:\n",
    "                # '입력' 시간과 '수정' 시간 중 첫 번째(입력 시간)를 선택\n",
    "                date_tag = date_area.find('span', class_='txt-date')\n",
    "                if date_tag:\n",
    "                    article_date = date_tag.get_text(strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_moneys(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('div', class_='article-body')\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['table', 'script', 'center']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            date_tag = soup.find('div', class_='date').find('time')\n",
    "            if date_tag and 'datetime' in date_tag.attrs:\n",
    "                article_date = date_tag['datetime']\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_viva100(url):\n",
    "    # 브릿지경제(viva100)\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('div', class_='news_content')\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['figure', 'div', 'script']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            date_area = soup.find('div', class_='news_date')\n",
    "            if date_area:\n",
    "                date_tag = date_area.find('span', text=re.compile('배포일'))\n",
    "                if date_tag:\n",
    "                    date_text = date_tag.get_text(strip=True)\n",
    "                    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2})', date_text)\n",
    "                    if match:\n",
    "                        article_date = match.group(1)\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_genews(url):\n",
    "    # 글로벌이코노믹\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('div', class_='vtxt detailCont')\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['div', 'figure', 'script', 'ins']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            date_area = soup.find('p', class_='r3')\n",
    "            if date_area:\n",
    "                # div.e1 영역의 텍스트를 통째로 가져온다\n",
    "                date_text_full = date_area.get_text(strip=True)\n",
    "                # 그 안에서 '입력' 다음에 오는 날짜 패턴을 정규식으로 찾는다\n",
    "                match = re.search(r'입력(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2})', date_text_full)\n",
    "                if match:\n",
    "                    article_date = match.group(1)\n",
    "                \n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_businesspost(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('div', class_='detail_editor')\n",
    "            if article_body:\n",
    "                # 불필요한 광고, 스크립트, 저작권 영역 제거\n",
    "                # 마지막 저작권 문구 제거\n",
    "                copyright_tag = article_body.find('div', class_='copyright')\n",
    "                if copyright_tag:\n",
    "                    copyright_tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "\n",
    "            # 날짜 수집\n",
    "            date_area = soup.find('div', class_='author_info')\n",
    "            if date_area:\n",
    "                # author_info div 안에 있는 span 태그들을 모두 찾아서 마지막 것을 선택\n",
    "                all_spans = date_area.find_all('span')\n",
    "                if all_spans:\n",
    "                    date_tag = all_spans[-1]\n",
    "                    article_date = date_tag.get_text(strip=True)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "        \n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_ceoscore(url):\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_body = soup.find('div', class_='article_content')\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['h3', 'div', 'iframe', 'script']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            date_area = soup.find('div', class_='date')\n",
    "            if date_area:\n",
    "                date_text = date_area.get_text()\n",
    "                match = re.search(r'입력\\s*(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2})', date_text)\n",
    "                if match:\n",
    "                    article_date = match.group(1)\n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)\n",
    "\n",
    "def crawl_common_type_j(url):\n",
    "    # 공통 함수 - 요기 포함되는 케이스가 제법 있었음\n",
    "    content_text, article_date = \"본문 수집 실패\", \"날짜 수집 실패\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 본문 수집\n",
    "            article_body = soup.find('article', id='article-view-content-div')\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['figure', 'div', 'style', 'script']):\n",
    "                    tag.decompose()\n",
    "                content_text = article_body.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 날짜 수집\n",
    "            # 1순위: <div class=\"info-group\">\n",
    "            date_area = soup.find('div', class_='info-group')\n",
    "            if not date_area:\n",
    "                # 2순위: <ul class=\"infomation\">\n",
    "                date_area = soup.find('ul', class_='infomation')\n",
    "            \n",
    "            if date_area:\n",
    "                # '입력' 또는 '승인' 텍스트를 포함하는 li 태그를 찾음\n",
    "                for li in date_area.find_all('li'):\n",
    "                    li_text = li.get_text()\n",
    "                    if '입력' in li_text or '승인' in li_text:\n",
    "                        match = re.search(r'(\\d{4}[-.]\\d{2}[-.]\\d{2}\\s\\d{2}:\\d{2})', li_text)\n",
    "                        if match:\n",
    "                            article_date = match.group(1).replace('.', '-')\n",
    "                            break # 날짜 찾으면 중단\n",
    "                            \n",
    "    except Exception as e:\n",
    "        content_text, article_date = f\"오류: {e}\", f\"오류: {e}\"\n",
    "    return (content_text, article_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb174b97-8dcf-4f5f-8919-f3fc92fd8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. 마스터 컨트롤러 및 설정\n",
    "# --------------------------------------------------------------------------\n",
    "# 언론사와 함수 매칭\n",
    "crawler_map = {\n",
    "    \"뉴스1\": crawl_news1, \n",
    "    \"뉴시스\": crawl_newsis,\n",
    "    \"연합뉴스\": crawl_yonhapnews,\n",
    "    \"파이낸셜뉴스\": crawl_fnnews, \n",
    "    \"이데일리\": crawl_edaily, \n",
    "    \"헤럴드경제\": crawl_heraldcorp,\n",
    "    \"한국경제\": crawl_hankyung,\n",
    "    \"뉴스핌\": crawl_newspim, \n",
    "    \"전자신문\": crawl_etnews, \n",
    "    \"디지털데일리\": crawl_ddaily,\n",
    "    \"조선일보\": crawl_chosun, \n",
    "    \"중앙일보\": crawl_joongang, \n",
    "    \"세계일보\": crawl_segye,\n",
    "    \"YTN\": crawl_ytn, \n",
    "    \"SBS Biz\": crawl_sbsbiz, \n",
    "    \"연합뉴스TV\": crawl_yonhapnewstv,\n",
    "    \"SBS\": crawl_sbs, \n",
    "    \"연합뉴스\": crawl_yonhapnews,\n",
    "    \"서울경제\": crawl_sedaily,\n",
    "    \"머니투데이\": crawl_mt,\n",
    "    \"매일경제\": crawl_mk,\n",
    "    \"EBN\": crawl_ebn,\n",
    "    \"머니S\": crawl_moneys,\n",
    "    \"이뉴스투데이\": crawl_common_type_j,\n",
    "    \"이코노믹리뷰\": crawl_common_type_j,\n",
    "    \"브릿지경제\": crawl_viva100,\n",
    "    \"CEO스코어데일리\": crawl_ceoscore,\n",
    "    \"파이낸셜포스트\": crawl_common_type_j,\n",
    "    \"글로벌이코노믹\": crawl_genews,\n",
    "    \"비즈니스포스트\": crawl_businesspost,\n",
    "    \"조선비즈\": crawl_chosunbiz_selenium,\n",
    "    \"아이뉴스24\": crawl_inews24,\n",
    "    \"디지털투데이\": crawl_common_type_j, \n",
    "    \"지디넷코리아\": crawl_zdnet,\n",
    "    \"테크M\": crawl_techm, \n",
    "    \"IT조선\": crawl_common_type_j,\n",
    "    \"디지털타임스\": crawl_dt,  \n",
    "    \"국민일보\": crawl_kmib,\n",
    "    \"동아일보\": crawl_donga,\n",
    "    \"서울신문\": crawl_seoul,\n",
    "    \"메트로신문\": crawl_metroseoul,\n",
    "    \"매일일보\": crawl_mi, \n",
    "    \"KBS\": crawl_kbs,\n",
    "    \"한국경제TV\": crawl_wowtv,\n",
    "    \"MBN\": crawl_mbn,\n",
    "    \"서울경제TV\": crawl_sentv,\n",
    "    \"데일리안\": crawl_dailian,\n",
    "    \"이투데이\": crawl_etoday,\n",
    "    \"아시아경제\": crawl_asiae,\n",
    "    \"노컷뉴스\": crawl_nocutnews,     \n",
    "    \"아주경제\": crawl_ajunews,\n",
    "    \"인사이트\": crawl_insight,\n",
    "    \"톱스타뉴스\": crawl_topstarnews,\n",
    "    \"뉴스웨이\": crawl_newsway,\n",
    "    \"뉴데일리\": crawl_newdaily,\n",
    "    \"포인트데일리\": crawl_common_type_j,\n",
    "    \"천지일보\": crawl_common_type_j,\n",
    "    \"핀포인트뉴스\": crawl_common_type_j,\n",
    "    \"데일리한국\": crawl_common_type_j,\n",
    "    \"서울와이어\": crawl_common_type_j,\n",
    "    \"마이데일리\": crawl_mydaily,\n",
    "    \"중앙이코노미뉴스\": crawl_common_type_j    \n",
    "}\n",
    "\n",
    "# [중간 저장 주기 설정]\n",
    "SAVE_INTERVAL = 100 # 100개마다 저장\n",
    "\n",
    "def get_content_and_date(row):\n",
    "    \"\"\"'언론사'에 맞는 크롤링 함수를 호출하고 (본문, 날짜) 튜플을 반환\"\"\"\n",
    "    press = row['언론사']\n",
    "    url = row['URL']\n",
    "    \n",
    "    crawler_function = crawler_map2.get(press)\n",
    "    \n",
    "    if crawler_function:\n",
    "        try:\n",
    "            # 0.5초 대기 후 함수 실행 (서버 부하 감소)\n",
    "            time.sleep(0.5)\n",
    "            return crawler_function(url)\n",
    "        except Exception as e:\n",
    "            return (f\"컨트롤러 오류: {e}\", f\"컨트롤러 오류: {e}\")\n",
    "    else:\n",
    "        return (\"해당 언론사 크롤러 없음\", \"해당 언론사 크롤러 없음\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 크롤링 실행 (apply, 중간 저장 및 이어하기 로직)\n",
    "# --------------------------------------------------------------------------\n",
    "# 출력 파일 경로 설정\n",
    "output_filename = \"crawled_press_phase2.pkl\"\n",
    "\n",
    "# --- 이어하기 로직 ---\n",
    "if os.path.exists(output_filename):\n",
    "    print(f\"'{output_filename}' 파일이 존재합니다. 작업을 이어서 시작합니다.\")\n",
    "    df_phase2 = pd.read_pickle(output_filename)\n",
    "else:\n",
    "    print(\"새로운 작업을 시작합니다.\")\n",
    "    df_phase2 = combined_df[combined_df['언론사'].isin(press_list_phase2)].copy()\n",
    "    # '본문'과 '기사작성일' 컬럼이 없으면 새로 만들고 비어있음(None)으로 초기화\n",
    "    if '본문' not in df_phase2.columns:\n",
    "        df_phase2['본문'] = None\n",
    "    if '기사작성일' not in df_phase2.columns:\n",
    "        df_phase2['기사작성일'] = None\n",
    "\n",
    "# 크롤링이 아직 되지 않은 행들만 대상으로 작업\n",
    "to_crawl = df_phase2[df_phase2['본문'].isnull()]\n",
    "\n",
    "if to_crawl.empty:\n",
    "    print(\"모든 기사 수집이 이미 완료되었습니다.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"★★★ 2단계 크롤링을 시작합니다 (대상: 17개 언론사) ★★★\")\n",
    "    print(f\"총 {len(df_phase2)}개 중 {len(to_crawl)}개의 남은 기사를 수집합니다. (PC를 켜두세요)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # --- apply와 tqdm을 함께 사용 ---\n",
    "    results = []\n",
    "    # tqdm으로 남은 작업량에 대한 진행률 바를 생성\n",
    "    for index, row in tqdm(to_crawl.iterrows(), total=len(to_crawl)):\n",
    "        # 각 행에 대해 마스터 함수를 실행\n",
    "        result_tuple = get_content_and_date(row)\n",
    "        results.append(result_tuple)\n",
    "        \n",
    "        # --- 중간 저장 로직 ---\n",
    "        current_progress_index = len(results)\n",
    "        if current_progress_index % SAVE_INTERVAL == 0:\n",
    "            # 지금까지 수집된 결과를 데이터프레임에 임시로 업데이트\n",
    "            temp_df = pd.DataFrame(results, index=to_crawl.index[:current_progress_index], columns=['본문', '기사작성일'])\n",
    "            df_phase2.update(temp_df)\n",
    "            # 중간 저장\n",
    "            df_phase2.to_pickle(output_filename)\n",
    "            tqdm.write(f\" -> 중간 저장 완료 ({current_progress_index}/{len(to_crawl)} 진행)\")\n",
    "\n",
    "    # 모든 크롤링이 끝난 후, 수집된 전체 결과를 데이터프레임에 최종 업데이트\n",
    "    final_df = pd.DataFrame(results, index=to_crawl.index, columns=['본문', '기사작성일'])\n",
    "    df_phase2.update(final_df)\n",
    "    \n",
    "    print(\"\\n★★★ 1단계 크롤링이 모두 완료되었습니다! ★★★\")\n",
    "\n",
    "    # 최종 결과 저장\n",
    "    df_phase2.to_pickle(output_filename)\n",
    "    print(f\"\\n1단계 크롤링 최종 결과를 '{output_filename}' 파일로 저장했습니다.\")\n",
    "\n",
    "df_phase2 = pd.read_pickle('crawled_press_phase2.pkl')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 결과 확인\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 기본 정보 확인\n",
    "df_phase2.head(5)\n",
    "df_phase2.shape\n",
    "df_phase2.info()\n",
    "\n",
    "# 날짜 컬럼에 '실패'나 '못했습니다' 같은 에러 메시지가 포함된 행 찾기\n",
    "error_mask = df_phase2['기사작성일'].astype(str).str.contains('실패|못했습니다', na=False)\n",
    "error_df = df_phase2[error_mask]\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"전체 수집 데이터: {len(df_phase2)}건\")\n",
    "print(f\"날짜 수집 실패 데이터: {len(error_df)}건\")\n",
    "print(\"-\" * 30)\n",
    "print(\"[ 언론사별 날짜 수집 실패 현황 ]\")\n",
    "print(error_df['언론사'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292ef47-73a4-430d-95e0-68075e824ea1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. 검색 키워드별 그룹핑을 위해 키워드 컬럼 추가\n",
    "목표 : 분석 작업시 전체 분석 외 세부 분석을 진행할 수 있도록 검색 키워드 컬럼을 추가합니다. \n",
    "핵심 : 세부 분석은 언론사 컬럼과 키워드 컬럼을 이용할 예정입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06a629-fcdb-4665-b77d-3e7a750e0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 기사들이 어떤 키워드로 조회되었던 내용인지 항목 추가 필요\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. setting\n",
    "# --------------------------------------------------------------------------\n",
    "# 5번 작업 결과 폴더 추가\n",
    "INPUT_PATH = \"E:/데일리프로젝트/251120_카카오톡 네이버기사,블로그/press_cleaned_data_news\" \n",
    "\n",
    "# 5번 작업 결과 파일명 추가\n",
    "PKL_FILE_PATH = \"crawled_press_phase2.pkl\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 원본 CSV 파일들을 이용해 [URL - 검색키워드] 족보 만들기\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"원본 파일에서 키워드 정보를 추출합니다...\")\n",
    "url_keyword_map = []\n",
    "\n",
    "if os.path.exists(INPUT_PATH):\n",
    "    for filename in os.listdir(INPUT_PATH):\n",
    "        if filename.endswith('.csv'):\n",
    "            # 파일명에서 키워드 추출\n",
    "            try:\n",
    "                keyword = filename.split('_', 5)[-1].replace('.csv', '')\n",
    "            except:\n",
    "                keyword = filename \n",
    "\n",
    "            temp_df = pd.read_csv(os.path.join(INPUT_PATH, filename))\n",
    "            for url in temp_df['URL']:\n",
    "                url_keyword_map.append({'URL': url, '검색키워드': keyword})\n",
    "else:\n",
    "    print(f\"오류: 입력 폴더 경로를 찾을 수 없습니다: {INPUT_PATH}\")\n",
    "\n",
    "keyword_df = pd.DataFrame(url_keyword_map).drop_duplicates(subset=['URL'])\n",
    "print(f\" -> 총 {len(keyword_df)}개의 URL에 대한 키워드 족보 확보 완료.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 기존 크롤링 결과(pkl) 불러오기 및 병합\n",
    "# --------------------------------------------------------------------------\n",
    "if os.path.exists(PKL_FILE_PATH):\n",
    "    df_main = pd.read_pickle(PKL_FILE_PATH)\n",
    "    print(f\"기존 데이터 {len(df_main)}건 로드 완료.\")\n",
    "    \n",
    "    # 기존 데이터에 '검색키워드' 컬럼 추가 (Left Join)\n",
    "    df_main = pd.merge(df_main, keyword_df, on='URL', how='left')\n",
    "    print(\" -> 기존 데이터에 검색 키워드 정보 결합 완료!\")\n",
    "else:\n",
    "    print(\"오류: 크롤링 결과 파일(pkl)이 없습니다.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. 최종 결과 저장\n",
    "# --------------------------------------------------------------------------\n",
    "# 덮어쓰지 않고 새 이름(_v2)으로 저장\n",
    "df_main.to_pickle(\"crawled_press_phase2_v2.pkl\")\n",
    "print(\"\\n최종 데이터 저장 완료: crawled_press_phase2_v2.pkl\")\n",
    "\n",
    "# 확인\n",
    "display(df_main.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56602c74-0b8d-4f67-8a4c-1a80496bfae7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 7. 파일별 자연어 처리 작업 진행\n",
    "목표 : 자연어 분석을 위해서 수집된 블로그 '제목과 본문'에서 특문, 이모티콘, 자음과 모음만 있는 경우를 제외하는 전처리 작업 진행합니다.\n",
    "핵심 : 숫자와 영문은 제거되지 않도록 처리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8e0e1-41f8-4f70-86b6-394e5b93e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# 자연어 분석을 위한 함수 만들기 & 테스트\n",
    "def clean_text_with_lib(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. [수정됨] emoji 라이브러리를 사용하여 모든 이모티콘을 찾아 빈 문자열로 바꾸기\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 2. 한글, 영어, 숫자를 제외한 특수문자 제거\n",
    "    text = re.sub(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
    "    \n",
    "    # 3. 자음 또는 모음만 반복되는 경우 제거 (ㅋㅋㅋ, ㅜㅜㅜ)\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1{2,}', '', text)\n",
    "    \n",
    "    # 4. 여러 개의 공백을 하나의 공백으로 변경\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 5. 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 테스트 코드 -> 결과 확인 후 반영\n",
    "test_review1 = \"업데이트 최악 1점 드립니다.ㅠㅠㅠ UI 돌려놔!!!! #카카오\"\n",
    "test_review2 = \"ㅋㅋㅋ Only salvation is Jesus, believe in Jesus 🙏\"\n",
    "test_review3 = \"이모티콘😄은 좋은데, 앱 속도가 너무 느려요...\"\n",
    "test_review4 = \"0점이 없어서 1점남김ㅋㅋ\"\n",
    "print(f\"원본: {test_review1} -> 정제 후: {clean_text_with_lib(test_review1)}\")\n",
    "print(f\"원본: {test_review2} -> 정제 후: {clean_text_with_lib(test_review2)}\")\n",
    "print(f\"원본: {test_review3} -> 정제 후: {clean_text_with_lib(test_review3)}\")\n",
    "print(f\"원본: {test_review4} -> 정제 후: {clean_text_with_lib(test_review4)}\")\n",
    "\n",
    "# 적용할 pkl 파일 호출\n",
    "df = pd.read_pickle(\"final_crawled_news_all.pkl\")\n",
    "\n",
    "df[\"제목_cleaned\"] = df[\"제목\"].apply(clean_text_with_lib)\n",
    "df[\"본문_cleaned\"] = df[\"본문\"].apply(clean_text_with_lib)\n",
    "\n",
    "# 최종 통합본 저장\n",
    "df.to_pickle(\"final_crawled_cleande_news_all.pkl\")\n",
    "print(\"\\n최종 통합 데이터를 'final_crawled_cleande_news_all.pkl' 파일로 저장했습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
