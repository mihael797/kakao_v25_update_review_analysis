{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13Vm0EaovhFJtJMpFHiGvdWc11KDxYAFK","authorship_tag":"ABX9TyOVMA8o1zrlYpSQ6OvGq3GO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# í†µí•© ì‹¬ì¸µ ë¶„ì„ íŒŒì´í”„ë¼ì¸\n","\n","1. í”„ë¡œì íŠ¸ ê°œìš”\n","\n","'ì¹´ì¹´ì˜¤í†¡ ì—…ë°ì´íŠ¸ ì‚¬ìš©ì í”¼ë°± ë¶„ì„' í”„ë¡œì íŠ¸ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì¸ ì‹¬ì¸µ ë¶„ì„(Deep-dive Analysis)ì„ ìˆ˜í–‰í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n","ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ ë° ì •ì œëœ ê° ë°ì´í„° ì†ŒìŠ¤(êµ¬ê¸€ ë¦¬ë·°, ë‰´ìŠ¤, ë¸”ë¡œê·¸)ë¥¼ ì¼ê´€ëœ ê¸°ì¤€ê³¼ ë°©ë²•ë¡ ì„ ì ìš©í•˜ì—¬ ë‹¤ê°ì ì¸ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n","\n","2. ì„¤ê³„ ë°©í–¥: ì„¤ì •ê³¼ ì‹¤í–‰ì˜ ë¶„ë¦¬\n","\n","ì‹¬ì¸µ ë¶„ì„ ì½”ë“œëŠ” <b>ì¬ì‚¬ìš©ì„±ê³¼ í™•ì¥ì„±</b>ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ë‘ ê°€ì§€ í•µì‹¬ ì›ì¹™ì— ë”°ë¼ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n","\n","- ëª¨ë“ˆí™”ëœ ë¶„ì„ í•¨ìˆ˜: í‚¤ì›Œë“œ ì¶”ì¶œ(Step 1)ë¶€í„° ë„¤íŠ¸ì›Œí¬ ë¶„ì„(Step 5)ê¹Œì§€, ê° ë¶„ì„ ë‹¨ê³„ë¥¼ <b>ë…ë¦½ì ì¸ í•¨ìˆ˜(Function)</b>ë¡œ ì •ì˜í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íŠ¹ì • ë¶„ì„ë§Œ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ë¶„ì„ ê¸°ë²•ì„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","- ì„¤ì • ê¸°ë°˜ íŒŒì´í”„ë¼ì¸: í•˜ë‹¨ì˜ 'ë¶„ì„ í™˜ê²½ ì„¤ì •' ì˜ì—­ì—ì„œ ANALYSIS_TARGET, SOURCE_DATA_PATH ë“± ëª‡ ê°€ì§€ íŒŒë¼ë¯¸í„°ë§Œ ë³€ê²½í•˜ë©´, ì •ì˜ëœ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°(êµ¬ê¸€ ë¦¬ë·°, ë‰´ìŠ¤, ë¸”ë¡œê·¸)ë“ , ì–´ë–¤ ë¶„ì„ êµ¬ê°„ì´ë“  ë™ì¼í•œ í’ˆì§ˆì˜ ì‹¬ì¸µ ë¶„ì„ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","3. ì£¼ìš” ë¶„ì„ ë‹¨ê³„\n","- Step 1. í‚¤ì›Œë“œ ì¶”ì¶œ: Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì„œì˜ í•µì‹¬ ëª…ì‚¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , ì´í›„ ë¶„ì„ì„ ìœ„í•´ .pkl íŒŒì¼ë¡œ ì¤‘ê°„ ì €ì¥í•©ë‹ˆë‹¤.\n","- Step 2. í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„: ê° ê°ì„± ê·¸ë£¹ë³„ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì‹œê°í™”í•˜ì—¬ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì£¼ì œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n","- Step 3. N-gram ë¶„ì„: ë‹¨ì–´ë“¤ì˜ ì—°ê²° ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ì‚¬ìš©ìë“¤ì´ ì‹¤ì œë¡œ ì‚¬ìš©í•œ êµ¬ì²´ì ì¸ 'êµ¬ë¬¸'ê³¼ 'ë§¥ë½'ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n","- Step 4. LDA í† í”½ ëª¨ë¸ë§: ë¬¸ì„œ êµ°ì§‘ì— ìˆ¨ì–´ìˆëŠ” í•µì‹¬ 'ì£¼ì œ(Topic)'ë“¤ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬, ì—¬ë¡ ì˜ ì£¼ìš” ìŸì ì„ êµ¬ì¡°ì ìœ¼ë¡œ íŒŒì•…í•©ë‹ˆë‹¤.\n","- Step 5. ë„¤íŠ¸ì›Œí¬ ë¶„ì„: í‚¤ì›Œë“œ ê°„ì˜ ë™ì‹œ ì¶œí˜„ ë¹ˆë„ë¥¼ ì‹œê°í™”í•˜ì—¬, ì–´ë–¤ í‚¤ì›Œë“œê°€ ì—¬ë¡ ì˜ 'ì¤‘ì‹¬'ì— ìˆê³ , ê° í‚¤ì›Œë“œë“¤ì´ ì–´ë–»ê²Œ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. *(ë¸”ë¡œê·¸ëŠ” Step 5 ì ìš©ì‹œ ë©”ëª¨ë¦¬ ì´ˆê³¼ ë¬¸ì œê°€ ë°œìƒë˜ì–´ ë¶„í• í•˜ì—¬ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì • ì½”ë“œë¥¼ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)*"],"metadata":{"id":"ukBK3ydF6hXv"}},{"cell_type":"code","source":["# ê·¸ë˜í”„ í•œê¸€ ë‚˜ëˆ” í°íŠ¸ ì„¤ì¹˜ ë° ì„¤ì •\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf\n","\n","# ëŸ°íƒ€ì„ > ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘ ì‹¤í–‰"],"metadata":{"id":"oh2yKrd8Ato1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ì›Œë“œ í´ë¼ìš°ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","!pip install konlpy wordcloud gensim pyLDAvis networkx tqdm nltk\n","\n","# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (punkt)\n","import nltk\n","nltk.download('punkt')"],"metadata":{"id":"7gRMB4ZEAT_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9qyEOW-0f96"},"outputs":[],"source":["# ==============================================================================\n","# setting\n","# ==============================================================================\n","\n","# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","from tqdm.auto import tqdm; tqdm.pandas()\n","import warnings\n","\n","# í…ìŠ¤íŠ¸ ë¶„ì„\n","from konlpy.tag import Okt\n","from collections import Counter\n","from itertools import combinations\n","from nltk import ngrams\n","import gensim\n","from gensim import corpora\n","\n","# ì‹œê°í™”\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","\n","# ê²½ê³  ë©”ì‹œì§€ë•Œë¬¸ì— ê²°ê³¼ ì •ë³´ í™•ì¸ì´ ì–´ë ¤ì›Œì„œ ìˆ¨ê¸°ê¸° ê¸°ëŠ¥ ì¶”ê°€\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"]},{"cell_type":"code","source":["# ==============================================================================\n","# 1 - 5 ë‹¨ê³„ë³„ ë¶„ì„/ì‹œê°í™” í•¨ìˆ˜ ì •ì˜(êµ¬ê¸€, ë‰´ìŠ¤, ë¸”ë¡œê·¸ ê³µí†µ)\n","# ==============================================================================\n","\n","# help : í•œê¸€ í°íŠ¸ í™•ì¸ ë° ì„¤ì • (í°íŠ¸ ì—†ìœ¼ë©´ ì¤‘ë‹¨)\n","def check_and_set_korean_font(stop_on_fail=True):\n","    # ì‹œê°í™”ë¥¼ ìœ„í•œ í•œê¸€ í°íŠ¸ í™•ì¸ í›„ í°íŠ¸ê°€ ì—†ì„ ê²½ìš° ë¶„ì„ ì¤‘ë‹¨\n","\n","    font_name = 'NanumBarunGothic'\n","\n","    if not any(font.name == font_name for font in fm.fontManager.ttflist):\n","        print(f\"[ê²½ê³ ] '{font_name}' í°íŠ¸ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n","        print(\"-> ì½”ë©(Colab) í™˜ê²½ì´ë¼ë©´, ìƒë‹¨ ë©”ë‰´ 'ëŸ°íƒ€ì„' -> 'ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘' í›„ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í°íŠ¸ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\")\n","        print(\"!sudo apt-get -y install fonts-nanum && sudo fc-cache -fv && rm ~/.cache/matplotlib -rf\")\n","\n","        if stop_on_fail:\n","            raise SystemExit(\"\\nâœ‹ í°íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. í°íŠ¸ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\")\n","\n","    else:\n","        print(f\"âœ… '{font_name}' í°íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œê°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\")\n","\n","    # í°íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ í™•ì¸ëœ ê²½ìš° ì„¤ì • ì‹¤í–‰\n","    plt.rc('font', family=font_name)\n","    plt.rcParams['axes.unicode_minus'] = False\n","\n","# help : ë‰´ìŠ¤/ë¸”ë¡œê·¸ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ë‚˜ëˆ ì„œ ë¶„ì„í•´ì•¼ í•´ì„œ ì²˜ë¦¬\n","def reshape_news_blog_data(df, title_col, title_sentiment_col, body_col, body_sentiment_col):\n","    # ë‰´ìŠ¤/ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ì‰½ê²Œ Long Formatìœ¼ë¡œ ë³€í™˜ í›„ ë³„ë„ í–‰ìœ¼ë¡œ ë¶„ë¦¬\n","\n","    print(\"ë‰´ìŠ¤/ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ë¶„ì„ìš© êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...\")\n","\n","    # 1. ì œëª© ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","    df_title = df[[title_col, title_sentiment_col]].copy()\n","    df_title.columns = ['text', 'sentiment']\n","    df_title['source_type'] = 'ì œëª©'\n","\n","    # 2. ë³¸ë¬¸ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","    df_body = df[[body_col, body_sentiment_col]].copy()\n","    df_body.columns = ['text', 'sentiment']\n","    df_body['source_type'] = 'ë³¸ë¬¸'\n","\n","    # 3. ë‘ ë°ì´í„°í”„ë ˆì„ì„ ìœ„ì•„ë˜ë¡œ í•©ì¹˜ê¸°\n","    df_reshaped = pd.concat([df_title, df_body], ignore_index=True)\n","\n","    # 4. ë¶„ì„ ê·¸ë£¹ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„± (ì˜ˆ: 'ì œëª©_ê¸ì •', 'ë³¸ë¬¸_ë¶€ì •')\n","    df_reshaped['analysis_group'] = df_reshaped['source_type'] + '_' + df_reshaped['sentiment']\n","\n","    # 5. ë¹„ì–´ìˆëŠ” í…ìŠ¤íŠ¸ í–‰ì€ ë¶„ì„ì—ì„œ ì œì™¸\n","    df_reshaped = df_reshaped.dropna(subset=['text'])\n","    df_reshaped = df_reshaped[df_reshaped['text'].str.strip() != '']\n","\n","    print(\"âœ… ë°ì´í„° êµ¬ì¡° ë³€í™˜ ì™„ë£Œ!\")\n","    return df_reshaped\n","\n","# [1ë‹¨ê³„] í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥ í•¨ìˆ˜\n","def step1_extract_and_save_keywords(df, text_columns, stopwords, output_path):\n","    print(f\"\\n[STEP 1] í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n","    okt = Okt()\n","    processed_df = df.copy()\n","    processed_df['í†µí•©ë³¸ë¬¸'] = processed_df[text_columns].fillna('').agg(' '.join, axis=1)\n","\n","    def extract_keywords(text):\n","        text = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£\\s]', '', str(text))\n","        nouns = okt.nouns(text)\n","        return [word for word in nouns if len(word) > 1 and word not in stopwords]\n","\n","    processed_df['keywords'] = processed_df['í†µí•©ë³¸ë¬¸'].progress_apply(extract_keywords)\n","    processed_df.to_pickle(output_path)\n","    print(f\"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","    return processed_df\n","\n","# [2ë‹¨ê³„] ë¹ˆë„ ë¶„ì„ (ë°ì´í„° ê³„ì‚°)\n","def step2_calculate_frequency(df_processed, sentiment_column):\n","    print(\"\\n[STEP 2] í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ë°ì´í„° ê³„ì‚° ì¤‘...\")\n","    results = {\n","        sentiment: Counter(sum(df_processed[df_processed[sentiment_column] == sentiment]['keywords'], []))\n","        for sentiment in sorted(df_processed[sentiment_column].unique())\n","    }\n","    print(\"âœ… ë¹ˆë„ ë°ì´í„° ê³„ì‚° ì™„ë£Œ!\")\n","    return results\n","\n","def visualize_frequency(sentiment, word_counts, top_n=20, save_prefix=\"\"):\n","    print(f\"  -> '{sentiment}' ê·¸ë£¹ ë¹ˆë„ ë¶„ì„ ì‹œê°í™”...\")\n","    most_common_words = word_counts.most_common(top_n)\n","    df_words = pd.DataFrame(most_common_words, columns=['word', 'count'])\n","\n","    plt.figure(figsize=(12, 8))\n","    sns.barplot(x='count', y='word', data=df_words, palette='viridis')\n","    plt.title(f'\"{save_prefix}{sentiment}\" ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ', fontsize=16)\n","    plt.show()\n","\n","    wordcloud = WordCloud(font_path='NanumBarunGothic', width=800, height=500, background_color='white').generate_from_frequencies(dict(word_counts))\n","    plt.figure(figsize=(10, 7))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.title(f'\"{save_prefix}{sentiment}\" ì›Œë“œ í´ë¼ìš°ë“œ', fontsize=16)\n","    plt.axis('off')\n","    plt.show()\n","\n","# [3ë‹¨ê³„] N-gram ë¶„ì„ (ë°ì´í„° ê³„ì‚° ë° ì‹œê°í™”)\n","def step3_analyze_and_visualize_ngrams(df_processed, sentiment_column, n=2, top_n=20, save_prefix=\"\"):\n","    print(f\"\\n[STEP 3] {n}-gram ë¶„ì„ ë° ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","    for sentiment in sorted(df_processed[sentiment_column].unique()):\n","        all_words = sum(df_processed[df_processed[sentiment_column] == sentiment]['keywords'], [])\n","        if len(all_words) < n: continue\n","\n","        ngrams_list = list(ngrams(all_words, n))\n","        ngram_counts = Counter(ngrams_list)\n","        most_common_ngrams = [(' '.join(gram), count) for gram, count in ngram_counts.most_common(top_n)]\n","\n","        df_ngrams = pd.DataFrame(most_common_ngrams, columns=[f'{n}-gram', 'count'])\n","\n","        print(f\"  -> '{sentiment}' ê·¸ë£¹ {n}-gram ì‹œê°í™”...\")\n","        plt.figure(figsize=(12, 8))\n","        sns.barplot(x='count', y=f'{n}-gram', data=df_ngrams, palette='plasma')\n","        plt.title(f'\"{save_prefix}{sentiment}\" ìƒìœ„ {top_n}ê°œ {n}-gram', fontsize=16)\n","        plt.show()\n","\n","# [4ë‹¨ê³„] LDA í† í”½ ëª¨ë¸ë§ (í•™ìŠµ ë° HTML ì €ì¥)\n","def step4_run_lda_modeling(df_processed, sentiment_column, num_topics=4, save_prefix=\"\"):\n","    print(\"\\n[STEP 4] LDA í† í”½ ëª¨ë¸ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","    for sentiment in sorted(df_processed[sentiment_column].unique()):\n","        print(f\"  -> '{sentiment}' ê·¸ë£¹ LDA ëª¨ë¸ í•™ìŠµ ë° ì €ì¥...\")\n","        documents = df_processed[df_processed[sentiment_column] == sentiment]['keywords'].tolist()\n","        documents = [doc for doc in documents if doc] # ë¹„ì–´ìˆëŠ” ë¦¬ìŠ¤íŠ¸ ì œê±°\n","        if not documents: continue\n","\n","        dictionary = corpora.Dictionary(documents)\n","        corpus = [dictionary.doc2bow(text) for text in documents]\n","\n","        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n","\n","        vis_data = gensimvis.prepare(ldamodel, corpus, dictionary, mds='tsne')\n","        html_file = f'{save_prefix}{sentiment}_LDA_ê²°ê³¼.html'\n","        pyLDAvis.save_html(vis_data, html_file)\n","        print(f\"    âœ… LDA ì‹œê°í™” íŒŒì¼ ì €ì¥ ì™„ë£Œ: {html_file}\")\n","\n","# [5ë‹¨ê³„] ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (ë°ì´í„° ê³„ì‚° ë° ì‹œê°í™”)\n","def step5_analyze_and_visualize_network(df_processed, sentiment_column, word_freq_results, top_n=50, save_prefix=\"\"):\n","    print(\"\\n[STEP 5] ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","    for sentiment in sorted(df_processed[sentiment_column].unique()):\n","        documents = df_processed[df_processed[sentiment_column] == sentiment]['keywords'].tolist()\n","        documents = [doc for doc in documents if doc]\n","        if not documents: continue\n","\n","        pairs = [combination for keywords in documents if len(keywords) >= 2 for combination in combinations(set(keywords), 2)]\n","        if not pairs: continue\n","\n","        pair_counts = Counter(pairs)\n","        df_edges = pd.DataFrame(pair_counts.most_common(top_n), columns=['pair', 'count'])\n","        df_edges[['source', 'target']] = pd.DataFrame(df_edges['pair'].tolist(), index=df_edges.index)\n","        G = nx.from_pandas_edgelist(df_edges, source='source', target='target', edge_attr='count')\n","\n","        print(f\"  -> '{sentiment}' ê·¸ë£¹ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”...\")\n","        plt.figure(figsize=(18, 18))\n","        pos = nx.spring_layout(G, k=0.8, iterations=100, seed=42)\n","        node_sizes = [np.sqrt(word_freq_results[sentiment].get(node, 1)) * 300 for node in G.nodes()]\n","        edge_widths = [d['count'] * 0.05 for (u, v, d) in G.edges(data=True)]\n","\n","        nx.draw_networkx(G, pos,\n","                         font_family='NanumBarunGothic', font_size=14,\n","                         node_size=node_sizes, node_color=list(dict(G.degree()).values()),\n","                         cmap=plt.cm.viridis, alpha=0.8,\n","                         width=edge_widths, edge_color='grey')\n","\n","        plt.title(f'\"{save_prefix}{sentiment}\" í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„', fontsize=25)\n","        plt.axis('off')\n","        plt.show()"],"metadata":{"id":"9F1qF73W0lno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [5ë‹¨ê³„] ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „) ë¸”ë¡œê·¸ ë¶„ì„ ì‹œ ì‚¬ìš©  >> ë¸”ë¡œê·¸ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì•ˆë˜ì„œ. ì¶”ê°€\n","\n","import gc # Garbage Collector ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€\n","\n","def step5_analyze_and_visualize_network_optimized(df_processed, sentiment_column, word_freq_results, top_n=40, save_prefix=\"\", chunk_size=10000):\n","    print(\"\\n[STEP 5] ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™” (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „)ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","    for sentiment in sorted(df_processed[sentiment_column].unique()):\n","        print(f\"  -> '{sentiment}' ê·¸ë£¹ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ê³„ì‚° ì¤‘...\")\n","        documents = df_processed[df_processed[sentiment_column] == sentiment]['keywords'].tolist()\n","        documents = [doc for doc in documents if doc]\n","        if not documents: continue\n","\n","        # ğŸ”¥ ì „ì²´ pairsë¥¼ í•œ ë²ˆì— ë§Œë“¤ì§€ ì•Šê³ , Counterë¥¼ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n","        pair_counts = Counter()\n","        for i in tqdm(range(0, len(documents), chunk_size), desc=\"  - ë‹¨ì–´ ìŒ ê³„ì‚° ì§„í–‰ë¥ \"):\n","            chunk_documents = documents[i:i+chunk_size]\n","            pairs_chunk = [\n","                combination for keywords in chunk_documents\n","                if len(keywords) >= 2\n","                for combination in combinations(set(keywords), 2)\n","            ]\n","            pair_counts.update(pairs_chunk)\n","            del pairs_chunk # ë©”ëª¨ë¦¬ì—ì„œ ì¦‰ì‹œ í•´ì œ\n","            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰\n","\n","        if not pair_counts: continue\n","\n","        df_edges = pd.DataFrame(pair_counts.most_common(top_n), columns=['pair', 'count'])\n","        df_edges[['source', 'target']] = pd.DataFrame(df_edges['pair'].tolist(), index=df_edges.index)\n","        G = nx.from_pandas_edgelist(df_edges, source='source', target='target', edge_attr='count')\n","\n","        print(f\"  -> '{sentiment}' ê·¸ë£¹ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”...\")\n","        plt.figure(figsize=(18, 18))\n","        pos = nx.spring_layout(G, k=0.8, iterations=100, seed=42)\n","        node_sizes = [np.sqrt(word_freq_results[sentiment].get(node, 1)) * 300 for node in G.nodes()]\n","        edge_widths = [d['count'] * 0.05 for (u, v, d) in G.edges(data=True)]\n","\n","        nx.draw_networkx(G, pos,\n","                         font_family='NanumBarunGothic', font_size=14,\n","                         node_size=node_sizes, node_color=list(dict(G.degree()).values()),\n","                         cmap=plt.cm.viridis, alpha=0.8,\n","                         width=edge_widths, edge_color='grey')\n","\n","        plt.title(f'\"{save_prefix}{sentiment}\" í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„', fontsize=25)\n","        plt.axis('off')\n","        plt.show()"],"metadata":{"id":"TTDhmFkp9PmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# [ìµœì¢…] ì‹¤ ì ìš© í™˜ê²½ ì„¤ì • ë° í•¨ìˆ˜ ì‹¤í–‰ ë¶€ë¶„ >> êµ¬ê¸€, ë‰´ìŠ¤, ë¸”ë¡œê·¸ ì„¸íŒ… ì§„í–‰\n","# ==============================================================================\n","\n","# 1. ë¶„ì„ í™˜ê²½ ì„¤ì •\n","\n","# [ êµ¬ê¸€ ìŠ¤í† ì–´ ë¦¬ë·° ]\n","ANALYSIS_TARGET = \"google_stroe\"\n","SOURCE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/251118_á„€á…®á„€á…³á†¯á„‰á…³á„á…©á„‹á…¥_á„€á…¡á†·á„‰á…¥á†¼á„‡á…®á†«á„‰á…¥á†¨á„€á…§á†¯á„€á…ª.csv' # ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n","PROCESSED_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/251201_á„€á…®á„€á…³á†¯á„…á…µá„‡á…²_keywords.pkl'      # ì¤‘ê°„ ì €ì¥ë  íŒŒì¼ ê²½ë¡œ\n","SAVE_PREFIX = 'êµ¬ê¸€ë¦¬ë·°_'                         # ê²°ê³¼ë¬¼ íŒŒì¼ ì´ë¦„ ì ‘ë‘ì‚¬\n","TEXT_COLUMNS = ['cleaned_content']              # ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ (2ê°œ ì´ìƒ ê°€ëŠ¥: ['ì œëª©', 'ë³¸ë¬¸'])\n","SENTIMENT_COLUMN = 'sentiment_integrated'       # ê°ì„± ë¶„ì„ ê²°ê³¼ ì»¬ëŸ¼\n","\n","# [ ë„¤ì´ë²„ ë‰´ìŠ¤-news, ë¸”ë¡œê·¸-blog ]\n","# ANALYSIS_TARGET = 'blog'\n","# SOURCE_DATA_PATH = 'ë¸”ë¡œê·¸_ë°ì´í„°_ê°ì„±ë¶„ì„ê²°ê³¼.csv'\n","# PROCESSED_DATA_PATH = 'ë¸”ë¡œê·¸_keywords.pkl'\n","# SAVE_PREFIX = 'ë¸”ë¡œê·¸_'\n","# # ì•„ë˜ ì»¬ëŸ¼ ì´ë¦„ë“¤ì€ ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n","# TITLE_COL = 'ì œëª©'\n","# TITLE_SENTIMENT_COL = 'ì œëª©_ê°ì„±'\n","# BODY_COL = 'ë³¸ë¬¸'\n","# BODY_SENTIMENT_COL = 'ë³¸ë¬¸_ê°ì„±'\n","\n","# ë¶„ì„ì— ì‚¬ìš©ë  ê³µí†µ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸\n","common_stopwords = [\n","    'ë“±', 'ìˆ˜', 'ì´', 'ê²ƒ', 'ì €', 'ê·¸', 'ë•Œ', 'ë°', 'ì˜', 'ë¥¼', 'ì—', 'ê°€', 'ë“¤', 'ì¢€', 'ì˜', 'ë”', 'ê·¸ëƒ¥', 'ê³¼',\n","    'ì¹´ì¹´ì˜¤', 'ì¹´í†¡', 'ì—…ë°ì´íŠ¸', 'ì—…ëƒ' , 'ì´ë²ˆ', 'ë²„ì „', 'ê¸°ì', 'ë‰´ìŠ¤', 'ì‚¬ì§„', 'ì œê³µ', 'ë¬´ë‹¨', 'ì „ì¬'\n","]\n","\n","# 2. ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n","\n","print(f\"--- '{SAVE_PREFIX}' ì „ì²´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---\")\n","try:\n","    check_and_set_korean_font(stop_on_fail=True)\n","except SystemExit as e:\n","    # í°íŠ¸ ë¬¸ì œë¡œ ì¤‘ë‹¨ë˜ì—ˆì„ ê²½ìš° ë©”ì‹œì§€ë¥¼ ì¶œë ¥ í›„ ì¢…ë£Œ\n","    print(e)\n","else:\n","    # í°íŠ¸ê°€ ì •ìƒì ì¼ ê²½ìš°ì—ë§Œ ì•„ë˜ì˜ ëª¨ë“  ë¶„ì„ ì‹¤í–‰\n","\n","    # [STEP 1] í‚¤ì›Œë“œ ì¶”ì¶œ\n","    if not os.path.exists(PROCESSED_DATA_PATH):\n","        print(f\"\\n[STEP 1] '{PROCESSED_DATA_PATH}' íŒŒì¼ì´ ì—†ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n","        df_raw = pd.read_csv(SOURCE_DATA_PATH)\n","            # ë¶„ì„ ëŒ€ìƒì— ë”°ë¼ ë°ì´í„° êµ¬ì¡° ê²°ì •\n","        if ANALYSIS_TARGET in ['blog', 'news']:\n","            df_to_process = reshape_news_blog_data(df_raw, TITLE_COL, TITLE_SENTIMENT_COL, BODY_COL, BODY_SENTIMENT_COL)\n","            # í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ì—ëŠ” ë³€í™˜ëœ ì»¬ëŸ¼ ì´ë¦„ì„ ì „ë‹¬\n","            df_processed = step1_extract_and_save_keywords(df_to_process, ['text'], common_stopwords, PROCESSED_DATA_PATH)\n","            # ë¶„ì„ ê·¸ë£¹(sentiment) ì»¬ëŸ¼ì„ ìƒˆë¡œ ë§Œë“  'analysis_group'ìœ¼ë¡œ ì§€ì •\n","            SENTIMENT_COLUMN = 'analysis_group'\n","        else: # êµ¬ê¸€ ë¦¬ë·° ë˜ëŠ” ê¸°íƒ€ ë‹¨ì¼ í…ìŠ¤íŠ¸ ë°ì´í„°\n","            df_processed = step1_extract_and_save_keywords(df_raw, TEXT_COLUMNS, common_stopwords, PROCESSED_DATA_PATH)\n","    else:\n","        print(f\"\\n[SKIP] ì´ë¯¸ '{PROCESSED_DATA_PATH}' íŒŒì¼ì´ ì¡´ì¬í•˜ì—¬ 1ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n","        df_processed = pd.read_pickle(PROCESSED_DATA_PATH)\n","        # ì €ì¥ëœ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ë•Œë„ ë¶„ì„ ëŒ€ìƒì— ë”°ë¼ SENTIMENT_COLUMNì„ ì¬ì •ì˜ í•´ì¤˜ì•¼ í•¨\n","        if ANALYSIS_TARGET in ['blog', 'news']:\n","            SENTIMENT_COLUMN = 'analysis_group'\n","\n","# [STEP 2] í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ë° ì‹œê°í™”\n","word_freq_results = step2_calculate_frequency(df_processed, SENTIMENT_COLUMN)\n","for sentiment, counts in word_freq_results.items():\n","    if counts:\n","        visualize_frequency(sentiment, counts, save_prefix=SAVE_PREFIX)\n","\n","# # [STEP 3] N-gram ë¶„ì„ ë° ì‹œê°í™”\n","step3_analyze_and_visualize_ngrams(df_processed, SENTIMENT_COLUMN, save_prefix=SAVE_PREFIX)\n","\n","# [STEP 4] LDA í† í”½ ëª¨ë¸ë§\n","step4_run_lda_modeling(df_processed, SENTIMENT_COLUMN, num_topics=4, save_prefix=SAVE_PREFIX)\n","\n","# [STEP 5] ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™”\n","# 2ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ ë‹¨ì–´ ë¹ˆë„ ê²°ê³¼ë¥¼ ë…¸ë“œ í¬ê¸° ì¡°ì ˆì— ì‚¬ìš©\n","step5_analyze_and_visualize_network(df_processed, SENTIMENT_COLUMN, word_freq_results, save_prefix=SAVE_PREFIX)\n","\n","print(\"\\n ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. \")"],"metadata":{"id":"rmthPyqd1K3g"},"execution_count":null,"outputs":[]}]}